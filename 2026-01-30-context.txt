
 ▐▛███▜▌   Claude Code v2.1.23
▝▜█████▛▘  Opus 4.5 · Claude Pro
  ▘▘ ▝▝    ~/Scaling-Laws/dplm

❯ is there a subdirectory called logs in this directory? 

⏺ Searched for 1 pattern (ctrl+o to expand)

⏺ No, there is no logs subdirectory in /Users/anujprabhu/Scaling-Laws/dplm.

❯ if I run run_generation_smoke_test.sh, should the logs directory be       
  created?                                                                  

⏺ Read 1 file (ctrl+o to expand)

⏺ No, run_generation_smoke_test.sh does not create the logs directory. Looking
  at lines 19-23:

  CHECKPOINT_DIR="logs"
  if [ ! -d "$CHECKPOINT_DIR" ]; then
      echo "Error: No logs directory found. Please run training first."
      exit 1
  fi

  The script expects the logs directory to already exist with a checkpoint
  inside. It will exit with an error if logs doesn't exist.

  The logs directory gets created when you run training (e.g., bash 
  scripts/run_smoke_test.sh). According to your CLAUDE.md, the smoke test
  training was completed on the wolf server and the checkpoint was saved to
  ~/data/dplm/logs/smoke_test_20251231_011053/checkpoints/.

  Since you're currently on your local machine (/Users/anujprabhu/...), you
  don't have the logs directory here. You would need to either:
  1. Run the generation script on the wolf server where the checkpoint exists
  2. Copy the checkpoint from the server to your local machine
  3. Run training locally first to create a checkpoint

❯ Last login: Tue Jan 27 14:28:34 on ttys000                                
  (base) anujprabhu@anujs-mbp ~ % ssh adprabh1@10.173.14.193                
  adprabh1@10.173.14.193's password:                                        
  (base) adprabh1@wolf:~$ cd data/dplm/                                     
  (base) adprabh1@wolf:~/data/dplm$ ls                                      
  analysis  CLAUDE.md  data-bin  generate_dplm2.py  generation-results      
  logs       requirements.txt  scripts    setup.py  test.py   vendor        
  assets    configs    env.yml   generate_dplm.py   LICENSE                 
  README.md  run               setup.cfg  src       train.py                
  (base) adprabh1@wolf:~/data/dplm$ cd logs/                                
  (base) adprabh1@wolf:~/data/dplm/logs$ ls                                 
  dplm_8m_tcr_2m_20260116_200127  smoke_test_20251231_005345                
  smoke_test_20251231_005839  smoke_test_20251231_010403                    
  smoke_test_20251231_010951                                                
  dplm_8m_tcr_2m_20260116_200410  smoke_test_20251231_005445                
  smoke_test_20251231_005950  smoke_test_20251231_010619                    
  smoke_test_20251231_011053                                                
  smoke_test_20251230_220225      smoke_test_20251231_005735                
  smoke_test_20251231_010251  smoke_test_20251231_010805                    
  smoke_test_20251231_151332                                                
  (base) adprabh1@wolf:~/data/dplm/logs$                                    
                                                                            

⏺ The logs directory exists on the wolf server with several training runs:

  - 2 TCR training runs from Jan 16, 2026:
    - dplm_8m_tcr_2m_20260116_200127
    - dplm_8m_tcr_2m_20260116_200410
  - Multiple smoke tests from Dec 30-31, 2025 (including the one mentioned in
  CLAUDE.md: smoke_test_20251231_011053)

  You're in the right place to run the generation smoke test. You can now:

  conda activate dplm
  bash scripts/run_generation_smoke_test.sh

  The script will automatically find the most recent checkpoint (likely from
  smoke_test_20251231_151332 based on the timestamp ordering) and run
  generation.

❯ how can I get more details about when the logs were created? I am         
  confused with the naming of the logs.                                     

⏺ The directory names include timestamps in YYYYMMDD_HHMMSS format, but you can
  verify actual creation times with:

  ls -lt

  This will sort by modification time (most recent first) and show dates.

  Or for more detailed info:

  ls -la

  Or to see the full timestamp for each directory:

  stat */

  The naming appears to follow the pattern
  {experiment_name}_{YYYYMMDD}_{HHMMSS}:
  - smoke_test_20251231_151332 → Dec 31, 2025 at 15:13:32
  - dplm_8m_tcr_2m_20260116_200127 → Jan 16, 2026 at 20:01:27

  Note: The dates 20260116 (Jan 16, 2026) in the TCR runs are in the future
  compared to today's date, which suggests either the server's clock was
  misconfigured or the naming convention uses a different date source.

❯ (base) adprabh1@wolf:~/data/dplm/logs/dplm_8m_tcr_2m_20260116_200410$ cat 
  train.log                                                                 
  [2026-01-16 20:04:15,528][datasets][INFO] - PyTorch version 2.2.0+cu121   
  available.                                                                
  [2026-01-16 20:04:16,779][root][INFO] - gcc -pthread -B                   
  /home/adprabh1/data/miniconda3/envs/dplm/compiler_compat                  
  -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem   
  /home/adprabh1/data/miniconda3/envs/dplm/include                          
  -I/home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -O2 -isystem     
  /home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -c                 
  /tmp/tmp10_f74_x/test.c -o /tmp/tmp10_f74_x/test.o                        
  [2026-01-16 20:04:16,821][root][INFO] - gcc -pthread -B                   
  /home/adprabh1/data/miniconda3/envs/dplm/compiler_compat                  
  /tmp/tmp10_f74_x/test.o -laio -o /tmp/tmp10_f74_x/a.out                   
  [2026-01-16 20:04:17,423][byprot.utils][INFO] - Current git revision      
  hash: dca34aaf7bfc0cd95b47d56011ef9ba38c6f3943                            
  [2026-01-16 20:04:17,424][byprot.utils][INFO] - Disabling python          
  warnings! <config.ignore_warnings=True>                                   
  [2026-01-16 20:04:17,424][byprot.utils][INFO] - Printing config tree with 
  Rich! <config.print_config=True>                                          
  [2026-01-16 20:04:17,424][byprot.utils][INFO] - Field 'training' not      
  found in config                                                           
  [2026-01-16 20:04:17,472][lightning_fabric.utilities.seed][INFO] - Seed   
  set to 42                                                                 
  [2026-01-16 20:04:17,472][byprot.training_pipeline][INFO] - Failed to     
  resume checkpoint from <last.ckpt>: file not exists. Skip.                
  [2026-01-16 20:04:17,472][byprot.utils][INFO] - Instantiating datamodule  
  <uniref50_hf>                                                             
  [2026-01-16 20:04:17,473][byprot.utils.config][INFO] -     Resolving      
  datamodule <uniref50_hf> ->                                               
  <byprot.datamodules.uniref50_hf.UniRefHFDataModule>                       
  [2026-01-16 20:04:17,474][byprot.utils][INFO] - Instantiating task        
  (pl_module) <lm/dplm>                                                     
  [2026-01-16 20:04:17,474][byprot.utils.config][INFO] -     Resolving task 
  <lm/dplm> -> <byprot.tasks.lm.dplm.DPLMTrainingTask>                      
  [2026-01-16 20:04:17,477][byprot.tasks.lm.dplm][INFO] - Instantiating     
  neural model <dplm>                                                       
  [2026-01-16 20:04:17,477][byprot.utils.config][INFO] -     Resolving      
  model <dplm> -> <byprot.models.dplm.dplm.DiffusionProteinLanguageModel>   
  [2026-01-16 20:04:18,433][byprot.utils][INFO] - Instantiating callback    
  <pytorch_lightning.callbacks.RichModelSummary>                            
  [2026-01-16 20:04:18,435][byprot.utils][INFO] - Instantiating callback    
  <byprot.utils.callbacks.ModelCheckpoint>                                  
  [2026-01-16 20:04:18,439][byprot.utils][INFO] - Instantiating callback    
  <pytorch_lightning.callbacks.EarlyStopping>                               
  [2026-01-16 20:04:18,440][byprot.training_pipeline][INFO] - Instantiating 
  trainer <pytorch_lightning.Trainer>                                       
  [2026-01-16 20:04:18,446][lightning.pytorch.utilities.rank_zero][INFO] -  
  Using 16bit Automatic Mixed Precision (AMP)                               
  [2026-01-16 20:04:18,447][lightning.pytorch.utilities.rank_zero][INFO] -  
  Trainer already configured with model summary callbacks: [<class          
  'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>].      
  Skipping setting a default `ModelSummary` callback.                       
  [2026-01-16 20:04:18,454][lightning.pytorch.utilities.rank_zero][INFO] -  
  GPU available: True (cuda), used: True                                    
  [2026-01-16 20:04:18,454][lightning.pytorch.utilities.rank_zero][INFO] -  
  TPU available: False, using: 0 TPU cores                                  
  [2026-01-16 20:04:18,454][lightning.pytorch.utilities.rank_zero][INFO] -  
  IPU available: False, using: 0 IPUs                                       
  [2026-01-16 20:04:18,454][lightning.pytorch.utilities.rank_zero][INFO] -  
  HPU available: False, using: 0 HPUs                                       
  [2026-01-16 20:04:18,455][byprot.training_pipeline][INFO] - Logging       
  hyperparameters!                                                          
  [2026-01-16 20:04:18,455][byprot.training_pipeline][INFO] - Starting      
  training!                                                                 
  (base) adprabh1@wolf:~/data/dplm/logs/dplm_8m_tcr_2m_20260116_200410$     
                                                                            

⏺ The log confirms the training ran on January 16, 2026 at 20:04:15 (the
  server's clock is set to 2026).

  The log shows the training initialized but ends right after "Starting
  training!" - it doesn't show any actual training progress. This could mean:

  1. Training failed early - check if there's an error further in the log or in
  stderr
  2. Training output went to stdout instead of the log file
  3. Training was interrupted

  To investigate further:

  # Check if there's a checkpoint
  ls -la checkpoints/

  # Check the full log size and end
  tail -50 train.log

  # Look for any error logs
  ls -la *.log

  # Check tensorboard logs if they exist
  ls -la tensorboard/ 2>/dev/null || ls -la */

  This will tell you if the training actually produced any results or if it
  failed during startup.

❯ a 

⏺ Did you mean to type something else? Let me know what you'd like to do next -
  I can help you:

  - Check the checkpoint status in that log directory
  - Look at other training runs
  - Run the generation smoke test
  - Something else?

❯ a lot of code was edited in this repo because openfold could not be       
  downloaded and the workaround was commenting/deleting/modifying code to   
  run initial pretraining/unconditional generation to then be able to scale 
  the model. look at the environment file, I am not sure if that was        
  modified as well to get code to run. however, when initializing conda     
  environment with env file, it failed to download openfold. Is it possible 
  to download openfold so that I do not have to modify a whole lot of       
  code? it is hard to track what code was modified and go back and try to   
  understand everything. Since this is a research project and a paper we    
  are trying to publish, I would rather modify as little code as possible   
  and still finish the project/scale the model. I do not mind restarting if 
  openfold can be downloaded and I can instantly start playing/running      
  models/checkpoints without modifying code.                                

⏺ Let me look at the environment file and understand the OpenFold situation
  better.

⏺ Searched for 1 pattern (ctrl+o to expand)

⏺ Let me check the install script and understand what code actually uses
  OpenFold.

⏺ Searched for 1 pattern (ctrl+o to expand)

⏺ This gives a clear picture. Let me check if OpenFold can be installed in a
  simpler way.

⏺ Bash(git diff 8a2e15e..HEAD --stat | head -50)
  ⎿  CLAUDE.md                                          | 495 +++++++++++++
    ++++++++
     configs/datamodule/tcr_2m.yaml                     |  36 ++
    … +30 lines (ctrl+o to expand)


⏺ Now I have the full picture. Here's the situation:

  Summary

  Files modified to work around OpenFold (31 files changed):
  - src/byprot/datamodules/__init__.py - exclude structure modules
  - src/byprot/models/__init__.py - exclude structure models
  - src/byprot/tasks/__init__.py - exclude structure tasks
  - src/byprot/datamodules/pdb_dataset/*.py - make OpenFold optional
  - vendor/openfold/setup.py - fix CUDA compilation

  The root problem: OpenFold's setup.py tries to compile a CUDA kernel
  (attn_core_inplace_cuda) which often fails due to CUDA version/GPU mismatches.

  Good news for your scaling experiments

  For DPLM sequence-only pretraining (your scaling laws experiments), OpenFold
  is used ONLY for:
  - residue_constants (amino acid definitions)
  - rigid_utils (coordinate utilities)

  These are pure Python - no CUDA compilation needed!

  Solution: Install OpenFold without CUDA kernels

  You can install OpenFold as a pure Python package by:

  # On wolf server, fresh conda environment
  conda create -n dplm_clean python=3.9 -y
  conda activate dplm_clean

  # Install PyTorch first
  pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url
  https://download.pytorch.org/whl/cu121

  # Install the main package
  cd ~/data/dplm
  pip install -e .

  # Install OpenFold WITHOUT compiling CUDA (trick: set CUDA_HOME to invalid 
  path)
  CUDA_HOME="" pip install -e vendor/openfold --no-build-isolation

  This forces the fallback to the CPU stub in setup.py (lines 105-115).

  Would you like me to:
  1. Create a clean install script that does this properly?
  2. Help you reset the repo to original and test this approach?
  3. Check if there's a PyPI version of OpenFold that works out of the box?

✻ Cogitated for 50s

❯ Can the root problem itself not be solved by trying to compile a CUDA     
  kernel correctly? Is this hard/a lot of work?                             

⏺ You raise a good point. Fixing the root cause would be cleaner than
  workarounds.

  The CUDA kernel compilation typically fails due to:
  1. Compute capability mismatch - The code targets old GPUs (sm_37, sm_52) but
  your L40S needs sm_89
  2. CUDA version incompatibility - Compiler flags that worked in CUDA 11 may
  fail in CUDA 12
  3. C++ standard issues - Older code may not compile with newer compilers

  Looking at the setup.py, a fix was already attempted (commit 0e2df1e) to
  target your GPU's compute capability (8.9 for L40S). But it may not have
  worked.

  To diagnose, we need to see the actual error. Can you try this on the wolf
  server?

  cd ~/data/dplm
  conda activate dplm

  # Check your CUDA setup
  nvcc --version
  nvidia-smi | head -5

  # Try to compile OpenFold and capture the error
  pip install -e vendor/openfold 2>&1 | tee openfold_install.log

  Then share the error output (or cat openfold_install.log).

  How hard is it to fix?
  - If it's just wrong compiler flags → Easy (minutes)
  - If it's a missing source file → Easy (add stub)
  - If it's a fundamental API incompatibility with CUDA 12 → Medium (hours, may
  need code changes)
  - If the kernel code uses deprecated CUDA APIs → Could be harder

  Once I see the actual error, I can tell you exactly what's needed. Often it's
  a one-line fix.

❯ (base) adprabh1@wolf:~/data/dplm$ cd ~/data/dplm                          
                                                                            
                                                                            
  conda activate dplm                                                       
                                                                            
                                                                            
                                                                            
                                                                            
                                                                            
  # Check your CUDA setup                                                   
                                                                            
                                                                            
  nvcc --version                                                            
                                                                            
                                                                            
  nvidia-smi | head -5                                                      
                                                                            
                                                                            
                                                                            
                                                                            
                                                                            
  # Try to compile OpenFold and capture the error                           
                                                                            
                                                                            
  pip install -e vendor/openfold 2>&1 | tee openfold_install.log            
                                                                            
  nvcc: NVIDIA (R) Cuda compiler driver                                     
  Copyright (c) 2005-2023 NVIDIA Corporation                                
  Built on Fri_Jan__6_16:45:21_PST_2023                                     
  Cuda compilation tools, release 12.0, V12.0.140                           
  Build cuda_12.0.r12.0/compiler.32267302_0                                 
  Tue Jan 27 23:51:05 2026                                                  
  +------------------------------------------------------------------------ 
  -----------------+                                                        
  | NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA   
  Version: 12.6     |                                                       
  |-----------------------------------------+------------------------+----- 
  -----------------+                                                        
  | GPU  Name                 Persistence-M | Bus-Id          Disp.A |      
  Volatile Uncorr. ECC |                                                    
  Obtaining file:///mnt/disk12/user/adprabh1/dplm/vendor/openfold           
  Installing build dependencies: started                                    
  Installing build dependencies: finished with status 'done'                
  Checking if build backend supports build_editable: started                
  Checking if build backend supports build_editable: finished with status   
  'done'                                                                    
  Getting requirements to build editable: started                           
  Getting requirements to build editable: finished with status 'error'      
  error: subprocess-exited-with-error                                       
                                                                            
  × Getting requirements to build editable did not run successfully.        
  │ exit code: 1                                                            
  ╰─> [19 lines of output]                                                  
  Traceback (most recent call last):                                        
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  389, in <module>                                                          
  main()                                                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  373, in main                                                              
  json_out["return_val"] = hook(**hook_input["kwargs"])                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  157, in get_requires_for_build_editable                                   
  return hook(config_settings)                                              
  File "/tmp/pip-build-env-8_qvb8n_/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 479, in                              
  get_requires_for_build_editable                                           
  return self.get_requires_for_build_wheel(config_settings)                 
  File "/tmp/pip-build-env-8_qvb8n_/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 333, in get_requires_for_build_wheel 
  return self._get_build_requires(config_settings,                          
  requirements=[])                                                          
  File "/tmp/pip-build-env-8_qvb8n_/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 301, in _get_build_requires          
  self.run_setup()                                                          
  File "/tmp/pip-build-env-8_qvb8n_/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 518, in run_setup                    
  super().run_setup(setup_script=setup_script)                              
  File "/tmp/pip-build-env-8_qvb8n_/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 317, in run_setup                    
  exec(code, locals())                                                      
  File "<string>", line 19, in <module>                                     
  ModuleNotFoundError: No module named 'torch'                              
  [end of output]                                                           
                                                                            
  note: This error originates from a subprocess, and is likely not a        
  problem with pip.                                                         
  ERROR: Failed to build                                                    
  'file:///mnt/disk12/user/adprabh1/dplm/vendor/openfold' when getting      
  requirements to build editable                                            
  (dplm) adprabh1@wolf:~/data/dplm$                                         
                                                                            

⏺ This is an easy fix! The error is not a CUDA compilation problem - it's a
  build isolation issue.

  The problem: pip creates an isolated build environment that doesn't have
  PyTorch, but setup.py imports torch at line 19.

  Solution: Use --no-build-isolation to use your existing environment's PyTorch:

  pip install -e vendor/openfold --no-build-isolation

  This should allow the setup.py to find torch and proceed with CUDA kernel
  compilation. Try it and let's see if there are any actual CUDA compilation
  errors after this.

❯ (dplm) adprabh1@wolf:~/data/dplm$ pip install -e vendor/openfold          
  --no-build-isolation                                                      
  Obtaining file:///mnt/disk12/user/adprabh1/dplm/vendor/openfold           
  Checking if build backend supports build_editable ... done                
  Preparing editable metadata (pyproject.toml) ... done                     
  Building wheels for collected packages: openfold                          
  Building editable for openfold (pyproject.toml) ... error                 
  error: subprocess-exited-with-error                                       
                                                                            
  × Building editable for openfold (pyproject.toml) did not run             
  successfully.                                                             
  │ exit code: 1                                                            
  ╰─> [123 lines of output]                                                 
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License           
  classifiers are deprecated.                                               
  !!                                                                        
                                                                            
  ***********************************************************               
  *********************                                                     
  Please consider removing the following classifiers in favor               
  of a SPDX license expression:                                             
                                                                            
  License :: OSI Approved :: Apache Software License                        
                                                                            
  See https://packaging.python.org/en/latest/guides/writing-p               
  yproject-toml/#license for details.                                       
  ***********************************************************               
  *********************                                                     
                                                                            
  !!                                                                        
  self._finalize_license_expression()                                       
  running editable_wheel                                                    
  creating                                                                  
  /tmp/pip-ephem-wheel-cache-izawj860/wheels/27/f9/57/eecdbafd35389aeed8509 
  fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.tmp-cq9s9nmw/openfold.egg-info 
  writing /tmp/pip-ephem-wheel-cache-izawj860/wheels/27/f9/57/eecdbaf       
  d35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.tmp-cq9s9nmw/ope 
  nfold.egg-info/PKG-INFO                                                   
  writing dependency_links to /tmp/pip-ephem-wheel-cache-izawj860/whe       
  els/27/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n 
  _7/.tmp-cq9s9nmw/openfold.egg-info/dependency_links.txt                   
  writing top-level names to /tmp/pip-ephem-wheel-cache-izawj860/whee       
  ls/27/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_ 
  7/.tmp-cq9s9nmw/openfold.egg-info/top_level.txt                           
  writing manifest file '/tmp/pip-ephem-wheel-cache-izawj860/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.t 
  mp-cq9s9nmw/openfold.egg-info/SOURCES.txt'                                
  reading manifest file '/tmp/pip-ephem-wheel-cache-izawj860/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.t 
  mp-cq9s9nmw/openfold.egg-info/SOURCES.txt'                                
  adding license file 'LICENSE'                                             
  writing manifest file '/tmp/pip-ephem-wheel-cache-izawj860/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.t 
  mp-cq9s9nmw/openfold.egg-info/SOURCES.txt'                                
  creating '/tmp/pip-ephem-wheel-cache-izawj860/wheels/27/f9/57/eecdb       
  afd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.tmp-cq9s9nmw/o 
  penfold-1.0.1.dist-info'                                                  
  creating /tmp/pip-ephem-wheel-cache-izawj860/wheels/27/f9/57/eecdba       
  fd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmp4ppi9n_7/.tmp-cq9s9nmw/op 
  enfold-1.0.1.dist-info/WHEEL                                              
  running build_py                                                          
  running build_ext                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/utils/cpp_extension.py:415: UserWarning: The detected CUDA        
  version (12.0) has a minor version mismatch with the version that was     
  used to compile PyTorch (12.1). Most likely this shouldn't be a problem.  
  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version,                 
  torch.version.cuda))                                                      
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/utils/cpp_extension.py:425: UserWarning: There are no g++ version 
  bounds defined for CUDA version 12.0                                      
  warnings.warn(f'There are no {compiler_name} version bounds               
  defined for CUDA version {cuda_str_version}')                             
  building 'attn_core_inplace_cuda' extension                               
  creating /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/csrc           
  Emitting ninja build file                                                 
  /tmp/tmpoa7tp63c.build-temp/build.ninja...                                
  Compiling objects...                                                      
  Allowing ninja to set a default number of workers... (overridable         
  by setting the environment variable MAX_JOBS=N)                           
  [1/2] c++ -MMD -MF                                                        
  /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/csrc/softmax_cuda.o.d   
  -pthread -B /home/adprabh1/data/miniconda3/envs/dplm/compiler_compat      
  -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem   
  /home/adprabh1/data/miniconda3/envs/dplm/include                          
  -I/home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -O2 -isystem     
  /home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -I/mnt/disk12/user 
  /adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc/ -I/home/adprab 
  h1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/torch/include    
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda.cpp -o                                                      
  /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/csrc/softmax_cuda.o -O3 
  -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5                        
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  -std=c++17                                                                
  [2/2] /usr/bin/nvcc --generate-dependencies-with-compile                  
  --dependency-output /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/csr 
  c/softmax_cuda_kernel.o.d -I/mnt/disk12/user/adprabh1/dplm/vendor/openfol 
  d/openfold/utils/kernel/csrc/ -I/home/adprabh1/data/miniconda3/envs/dplm/ 
  lib/python3.9/site-packages/torch/include                                 
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda_kernel.cu -o /tmp/tmpoa7tp63c.build-temp/openfold/utils/ker 
  nel/csrc/softmax_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__               
  -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__         
  -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options 
  ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3   
  -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ 
  -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr                   
  --expt-extended-lambda -gencode arch=compute_89,code=sm_89                
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  FAILED: [code=1] /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/       
  csrc/softmax_cuda_kernel.o                                                
  /usr/bin/nvcc --generate-dependencies-with-compile                        
  --dependency-output /tmp/tmpoa7tp63c.build-temp/openfold/utils/kernel/csr 
  c/softmax_cuda_kernel.o.d -I/mnt/disk12/user/adprabh1/dplm/vendor/openfol 
  d/openfold/utils/kernel/csrc/ -I/home/adprabh1/data/miniconda3/envs/dplm/ 
  lib/python3.9/site-packages/torch/include                                 
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda_kernel.cu -o /tmp/tmpoa7tp63c.build-temp/openfold/utils/ker 
  nel/csrc/softmax_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__               
  -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__         
  -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options 
  ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3   
  -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ 
  -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr                   
  --expt-extended-lambda -gencode arch=compute_89,code=sm_89                
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h: In function ‘typename          
  pybind11::detail::type_caster<typename                                    
  pybind11::detail::intrinsic_type<T>::type>::cast_op_type<T>               
  pybind11::detail::cast_op(make_caster<T>&)’:                              
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:120: error: expected         
  template-name before ‘<’ token                                            
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:120: error: expected         
  identifier before ‘<’ token                                               
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:123: error: expected         
  primary-expression before ‘>’ token                                       
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:126: error: expected         
  primary-expression before ‘)’ token                                       
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  ninja: build stopped: subcommand failed.                                  
  Traceback (most recent call last):                                        
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 2096, in _run_ninja_build   
  subprocess.run(                                                           
  File                                                                      
  "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/subprocess.py",   
  line 528, in run                                                          
  raise CalledProcessError(retcode, process.args,                           
  subprocess.CalledProcessError: Command '['ninja', '-v']' returned         
  non-zero exit status 1.                                                   
                                                                            
  The above exception was the direct cause of the following                 
  exception:                                                                
                                                                            
  Traceback (most recent call last):                                        
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  389, in <module>                                                          
  main()                                                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  373, in main                                                              
  json_out["return_val"] = hook(**hook_input["kwargs"])                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  303, in build_editable                                                    
  return hook(wheel_directory, config_settings,                             
  metadata_directory)                                                       
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 468, in build_editable          
  return self._build_with_temp_dir(                                         
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 404, in _build_with_temp_dir    
  self.run_setup()                                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 512, in run_setup               
  super().run_setup(setup_script=setup_script)                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 317, in run_setup               
  exec(code, locals())                                                      
  File "<string>", line 117, in <module>                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/__init__.py", line 115, in setup                     
  return distutils.core.setup(**attrs)                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/core.py", line 186, in setup              
  return run_commands(dist)                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/core.py", line 202, in run_commands       
  dist.run_commands()                                                       
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1002, in run_commands      
  self.run_command(cmd)                                                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/dist.py", line 1102, in run_command                  
  super().run_command(command)                                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1021, in run_command       
  cmd_obj.run()                                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 139, in run         
  self._create_wheel_file(bdist_wheel)                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 349, in             
  _create_wheel_file                                                        
  files, mapping = self._run_build_commands(dist_name, unpacked,            
  lib, tmp)                                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 272, in             
  _run_build_commands                                                       
  self._run_build_subcommands()                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 299, in             
  _run_build_subcommands                                                    
  self.run_command(name)                                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/cmd.py", line 357, in run_command         
  self.distribution.run_command(command)                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/dist.py", line 1102, in run_command                  
  super().run_command(command)                                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1021, in run_command       
  cmd_obj.run()                                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/build_ext.py", line 96, in run               
  _build_ext.run(self)                                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 368, in run   
  self.build_extensions()                                                   
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 871, in build_extensions    
  build_ext.build_extensions(self)                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 484, in       
  build_extensions                                                          
  self._build_extensions_serial()                                           
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 510, in       
  _build_extensions_serial                                                  
  self.build_extension(ext)                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/build_ext.py", line 261, in build_extension  
  _build_ext.build_extension(self, ext)                                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 565, in       
  build_extension                                                           
  objects = self.compiler.compile(                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 684, in                     
  unix_wrap_ninja_compile                                                   
  _write_ninja_file_and_compile_objects(                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 1774, in                    
  _write_ninja_file_and_compile_objects                                     
  _run_ninja_build(                                                         
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 2112, in _run_ninja_build   
  raise RuntimeError(message) from e                                        
  RuntimeError: Error compiling objects for extension                       
  [end of output]                                                           
                                                                            
  note: This error originates from a subprocess, and is likely not a        
  problem with pip.                                                         
  ERROR: Failed building editable for openfold                              
  Failed to build openfold                                                  
  error: failed-wheel-build-for-install                                     
                                                                            
  × Failed to build installable wheels for some pyproject.toml based        
  projects                                                                  
  ╰─> openfold                                                              
  (dplm) adprabh1@wolf:~/data/dplm$                                         

⏺ Now we see the real issue. This is a pybind11/nvcc compatibility bug - nvcc
  (CUDA compiler) can't parse certain C++ template syntax in PyTorch's bundled
  pybind11 headers.

  This is a known issue and is harder to fix directly (would require patching
  pybind11 or upgrading PyTorch).

  However, for your use case (DPLM sequence-only), you don't need this CUDA
  kernel at all. The attn_core_inplace_cuda kernel is for optimized attention in
   AlphaFold's structure module - not used in sequence generation.

  Quick fix - force CPU-only installation:

  CUDA_HOME="" pip install -e vendor/openfold --no-build-isolation

  This tricks setup.py into using the CPU stub instead of compiling CUDA code.

  If you want to fix the CUDA compilation properly (for future DPLM-2 work),
  you'd need to either:
  1. Upgrade PyTorch to a version with fixed pybind11 (2.3+ might work)
  2. Or downgrade to an older CUDA toolkit (11.x) that has fewer nvcc parsing
  issues

  For now, I'd recommend the CPU-only path since it gets you unblocked
  immediately for your scaling experiments. Want to try that?

❯ when would I need the CUDA kernel? also, yes let's try the CPU-only path 

⏺ When you'd need the CUDA kernel

  The attn_core_inplace_cuda kernel is for optimized attention in structure 
  prediction. You'd need it for:

  - DPLM-2 (sequence + structure co-generation)
  - Inverse folding (structure → sequence)
  - Forward folding (sequence → structure)
  - Structure quality evaluation using OpenFold directly

  What you're doing (scaling laws) - NO CUDA kernel needed

  - DPLM sequence-only pretraining ✓
  - Unconditional sequence generation ✓
  - Model/data scaling experiments ✓
  - pLDDT evaluation (uses ESMFold, not OpenFold) ✓

  You only need OpenFold's Python utilities (residue_constants, rigid_utils) -
  no CUDA required.

  ---
  Now try the CPU-only installation:

  CUDA_HOME="" pip install -e vendor/openfold --no-build-isolation

❯ (dplm) adprabh1@wolf:~/data/dplm$   CUDA_HOME="" pip install -e           
  vendor/openfold --no-build-isolation                                      
                                                                            
  Obtaining file:///mnt/disk12/user/adprabh1/dplm/vendor/openfold           
  Checking if build backend supports build_editable ... done                
  Preparing editable metadata (pyproject.toml) ... done                     
  Building wheels for collected packages: openfold                          
  Building editable for openfold (pyproject.toml) ... error                 
  error: subprocess-exited-with-error                                       
                                                                            
  × Building editable for openfold (pyproject.toml) did not run             
  successfully.                                                             
  │ exit code: 1                                                            
  ╰─> [123 lines of output]                                                 
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License           
  classifiers are deprecated.                                               
  !!                                                                        
                                                                            
  ***********************************************************               
  *********************                                                     
  Please consider removing the following classifiers in favor               
  of a SPDX license expression:                                             
                                                                            
  License :: OSI Approved :: Apache Software License                        
                                                                            
  See https://packaging.python.org/en/latest/guides/writing-p               
  yproject-toml/#license for details.                                       
  ***********************************************************               
  *********************                                                     
                                                                            
  !!                                                                        
  self._finalize_license_expression()                                       
  running editable_wheel                                                    
  creating                                                                  
  /tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/27/f9/57/eecdbafd35389aeed8509 
  fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.tmp-k88_89nb/openfold.egg-info 
  writing /tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/27/f9/57/eecdbaf       
  d35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.tmp-k88_89nb/ope 
  nfold.egg-info/PKG-INFO                                                   
  writing dependency_links to /tmp/pip-ephem-wheel-cache-a3i7e1e_/whe       
  els/27/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvt 
  l6/.tmp-k88_89nb/openfold.egg-info/dependency_links.txt                   
  writing top-level names to /tmp/pip-ephem-wheel-cache-a3i7e1e_/whee       
  ls/27/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl 
  6/.tmp-k88_89nb/openfold.egg-info/top_level.txt                           
  writing manifest file '/tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.t 
  mp-k88_89nb/openfold.egg-info/SOURCES.txt'                                
  reading manifest file '/tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.t 
  mp-k88_89nb/openfold.egg-info/SOURCES.txt'                                
  adding license file 'LICENSE'                                             
  writing manifest file '/tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/2       
  7/f9/57/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.t 
  mp-k88_89nb/openfold.egg-info/SOURCES.txt'                                
  creating '/tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/27/f9/57/eecdb       
  afd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.tmp-k88_89nb/o 
  penfold-1.0.1.dist-info'                                                  
  creating /tmp/pip-ephem-wheel-cache-a3i7e1e_/wheels/27/f9/57/eecdba       
  fd35389aeed8509fb9c3309b5adc37c54480ae62535d/tmpvu0yvtl6/.tmp-k88_89nb/op 
  enfold-1.0.1.dist-info/WHEEL                                              
  running build_py                                                          
  running build_ext                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/utils/cpp_extension.py:415: UserWarning: The detected CUDA        
  version (12.0) has a minor version mismatch with the version that was     
  used to compile PyTorch (12.1). Most likely this shouldn't be a problem.  
  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version,                 
  torch.version.cuda))                                                      
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/utils/cpp_extension.py:425: UserWarning: There are no g++ version 
  bounds defined for CUDA version 12.0                                      
  warnings.warn(f'There are no {compiler_name} version bounds               
  defined for CUDA version {cuda_str_version}')                             
  building 'attn_core_inplace_cuda' extension                               
  creating /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/csrc           
  Emitting ninja build file                                                 
  /tmp/tmpgzbakr9_.build-temp/build.ninja...                                
  Compiling objects...                                                      
  Allowing ninja to set a default number of workers... (overridable         
  by setting the environment variable MAX_JOBS=N)                           
  [1/2] c++ -MMD -MF                                                        
  /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/csrc/softmax_cuda.o.d   
  -pthread -B /home/adprabh1/data/miniconda3/envs/dplm/compiler_compat      
  -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem   
  /home/adprabh1/data/miniconda3/envs/dplm/include                          
  -I/home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -O2 -isystem     
  /home/adprabh1/data/miniconda3/envs/dplm/include -fPIC -I/mnt/disk12/user 
  /adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc/ -I/home/adprab 
  h1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/torch/include    
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda.cpp -o                                                      
  /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/csrc/softmax_cuda.o -O3 
  -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5                        
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  -std=c++17                                                                
  [2/2] /usr/bin/nvcc --generate-dependencies-with-compile                  
  --dependency-output /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/csr 
  c/softmax_cuda_kernel.o.d -I/mnt/disk12/user/adprabh1/dplm/vendor/openfol 
  d/openfold/utils/kernel/csrc/ -I/home/adprabh1/data/miniconda3/envs/dplm/ 
  lib/python3.9/site-packages/torch/include                                 
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda_kernel.cu -o /tmp/tmpgzbakr9_.build-temp/openfold/utils/ker 
  nel/csrc/softmax_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__               
  -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__         
  -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options 
  ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3   
  -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ 
  -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr                   
  --expt-extended-lambda -gencode arch=compute_89,code=sm_89                
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  FAILED: [code=1] /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/       
  csrc/softmax_cuda_kernel.o                                                
  /usr/bin/nvcc --generate-dependencies-with-compile                        
  --dependency-output /tmp/tmpgzbakr9_.build-temp/openfold/utils/kernel/csr 
  c/softmax_cuda_kernel.o.d -I/mnt/disk12/user/adprabh1/dplm/vendor/openfol 
  d/openfold/utils/kernel/csrc/ -I/home/adprabh1/data/miniconda3/envs/dplm/ 
  lib/python3.9/site-packages/torch/include                                 
  -I/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-packages/to 
  rch/include/torch/csrc/api/include -I/home/adprabh1/data/miniconda3/envs/ 
  dplm/lib/python3.9/site-packages/torch/include/TH -I/home/adprabh1/data/m 
  iniconda3/envs/dplm/lib/python3.9/site-packages/torch/include/THC         
  -I/home/adprabh1/data/miniconda3/envs/dplm/include/python3.9 -c -c        
  /mnt/disk12/user/adprabh1/dplm/vendor/openfold/openfold/utils/kernel/csrc 
  /softmax_cuda_kernel.cu -o /tmp/tmpgzbakr9_.build-temp/openfold/utils/ker 
  nel/csrc/softmax_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__               
  -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__         
  -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options 
  ''"'"'-fPIC'"'"'' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3   
  -DVERSION_GE_1_5 -std=c++17 -maxrregcount=50 -U__CUDA_NO_HALF_OPERATORS__ 
  -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr                   
  --expt-extended-lambda -gencode arch=compute_89,code=sm_89                
  -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"'         
  '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1011"'     
  -DTORCH_EXTENSION_NAME=attn_core_inplace_cuda -D_GLIBCXX_USE_CXX11_ABI=0  
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h: In function ‘typename          
  pybind11::detail::type_caster<typename                                    
  pybind11::detail::intrinsic_type<T>::type>::cast_op_type<T>               
  pybind11::detail::cast_op(make_caster<T>&)’:                              
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:120: error: expected         
  template-name before ‘<’ token                                            
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:120: error: expected         
  identifier before ‘<’ token                                               
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:123: error: expected         
  primary-expression before ‘>’ token                                       
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  /home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site-package       
  s/torch/include/pybind11/detail/../cast.h:45:126: error: expected         
  primary-expression before ‘)’ token                                       
  45 |     return caster.operator typename                                  
  make_caster<T>::template cast_op_type<T>();                               
  |                                                                         
  ^                                                                         
  ninja: build stopped: subcommand failed.                                  
  Traceback (most recent call last):                                        
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 2096, in _run_ninja_build   
  subprocess.run(                                                           
  File                                                                      
  "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/subprocess.py",   
  line 528, in run                                                          
  raise CalledProcessError(retcode, process.args,                           
  subprocess.CalledProcessError: Command '['ninja', '-v']' returned         
  non-zero exit status 1.                                                   
                                                                            
  The above exception was the direct cause of the following                 
  exception:                                                                
                                                                            
  Traceback (most recent call last):                                        
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  389, in <module>                                                          
  main()                                                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  373, in main                                                              
  json_out["return_val"] = hook(**hook_input["kwargs"])                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line   
  303, in build_editable                                                    
  return hook(wheel_directory, config_settings,                             
  metadata_directory)                                                       
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 468, in build_editable          
  return self._build_with_temp_dir(                                         
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 404, in _build_with_temp_dir    
  self.run_setup()                                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 512, in run_setup               
  super().run_setup(setup_script=setup_script)                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/build_meta.py", line 317, in run_setup               
  exec(code, locals())                                                      
  File "<string>", line 117, in <module>                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/__init__.py", line 115, in setup                     
  return distutils.core.setup(**attrs)                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/core.py", line 186, in setup              
  return run_commands(dist)                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/core.py", line 202, in run_commands       
  dist.run_commands()                                                       
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1002, in run_commands      
  self.run_command(cmd)                                                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/dist.py", line 1102, in run_command                  
  super().run_command(command)                                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1021, in run_command       
  cmd_obj.run()                                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 139, in run         
  self._create_wheel_file(bdist_wheel)                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 349, in             
  _create_wheel_file                                                        
  files, mapping = self._run_build_commands(dist_name, unpacked,            
  lib, tmp)                                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 272, in             
  _run_build_commands                                                       
  self._run_build_subcommands()                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/editable_wheel.py", line 299, in             
  _run_build_subcommands                                                    
  self.run_command(name)                                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/cmd.py", line 357, in run_command         
  self.distribution.run_command(command)                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/dist.py", line 1102, in run_command                  
  super().run_command(command)                                              
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/dist.py", line 1021, in run_command       
  cmd_obj.run()                                                             
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/build_ext.py", line 96, in run               
  _build_ext.run(self)                                                      
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 368, in run   
  self.build_extensions()                                                   
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 871, in build_extensions    
  build_ext.build_extensions(self)                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 484, in       
  build_extensions                                                          
  self._build_extensions_serial()                                           
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 510, in       
  _build_extensions_serial                                                  
  self.build_extension(ext)                                                 
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/command/build_ext.py", line 261, in build_extension  
  _build_ext.build_extension(self, ext)                                     
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/setuptools/_distutils/command/build_ext.py", line 565, in       
  build_extension                                                           
  objects = self.compiler.compile(                                          
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 684, in                     
  unix_wrap_ninja_compile                                                   
  _write_ninja_file_and_compile_objects(                                    
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 1774, in                    
  _write_ninja_file_and_compile_objects                                     
  _run_ninja_build(                                                         
  File "/home/adprabh1/data/miniconda3/envs/dplm/lib/python3.9/site         
  -packages/torch/utils/cpp_extension.py", line 2112, in _run_ninja_build   
  raise RuntimeError(message) from e                                        
  RuntimeError: Error compiling objects for extension                       
  [end of output]                                                           
                                                                            
  note: This error originates from a subprocess, and is likely not a        
  problem with pip.                                                         
  ERROR: Failed building editable for openfold                              
  Failed to build openfold                                                  
  error: failed-wheel-build-for-install                                     
                                                                            
  × Failed to build installable wheels for some pyproject.toml based        
  projects                                                                  
  ╰─> openfold                                                              
  (dplm) adprabh1@wolf:~/data/dplm$                                         
                                                                            

⏺ The CUDA_HOME="" trick didn't work because PyTorch still detects CUDA via
  torch.version.cuda. The setup.py check requires BOTH conditions to be false.

  Let me fix the setup.py directly to force CPU-only mode:

⏺ Read 1 file (ctrl+o to expand)

⏺ I see the issue. The check at line 41 requires torch.version.cuda==None too,
  but PyTorch has CUDA compiled in. Let me add an environment variable to force
  CPU-only mode:

⏺ Update(vendor/openfold/setup.py)
  ⎿  Added 4 lines
      38  ]
      39  
      40  def get_cuda_bare_metal_version(cuda_dir):
      41 +    # Allow forcing CPU-only build via environment variable    
      42 +    if os.environ.get("OPENFOLD_CPU_ONLY", "0") == "1":        
      43 +        print("OPENFOLD_CPU_ONLY=1 set, installing CPU-only ver
         +sion")                                                         
      44 +        return None, -1, 0                                     
      45      if cuda_dir==None or torch.version.cuda==None:
      46          print("CUDA is not found, cpu version is installed")
      47          return None, -1, 0

⏺ I've modified your local copy. Now you need to make the same change on the
  wolf server. Run this on wolf:

  cd ~/data/dplm/vendor/openfold

  # Add the CPU-only option to setup.py
  sed -i 's/def get_cuda_bare_metal_version(cuda_dir):/def 
  get_cuda_bare_metal_version(cuda_dir):\n    # Allow forcing CPU-only build via
   environment variable\n    if os.environ.get("OPENFOLD_CPU_ONLY", "0") == 
  "1":\n        print("OPENFOLD_CPU_ONLY=1 set, installing CPU-only version")\n 
         return None, -1, 0/' setup.py

  Or simpler - just edit line 40-43 manually with nano or vim:

  nano ~/data/dplm/vendor/openfold/setup.py

  Add these lines right after def get_cuda_bare_metal_version(cuda_dir): (before
   the existing if statement):

      # Allow forcing CPU-only build via environment variable
      if os.environ.get("OPENFOLD_CPU_ONLY", "0") == "1":
          print("OPENFOLD_CPU_ONLY=1 set, installing CPU-only version")
          return None, -1, 0

  Then install with:

  cd ~/data/dplm
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

❯ I have a git repo in this directory. The way I have been updating code    
  both locally and in the wolf server is by pushing everything to origin    
  and pulling on wolf sever. I edit code locally because of increased       
  support vs vim/nano/emacs etc in a terminal session                       

⏺ That makes sense. Since I already edited the file locally, you just need to:

  1. Locally - commit and push:
  cd /Users/anujprabhu/Scaling-Laws/dplm
  git add vendor/openfold/setup.py
  git commit -m "Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel compilation"
  git push

  2. On wolf - pull and install:
  cd ~/data/dplm
  git pull
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

❯ (base) anujprabhu@anujs-mbp dplm % d /Users/anujprabhu/Scaling-Laws/dplm  
                                                                            
  git add vendor/openfold/setup.py                                          
                                                                            
  git commit -m "Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel          
  compilation"                                                              
  git push                                                                  
  zsh: command not found: d                                                 
  [main 25555f6] Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel          
  compilation                                                               
  1 file changed, 4 insertions(+)                                           
  Enumerating objects: 9, done.                                             
  Counting objects: 100% (9/9), done.                                       
  Delta compression using up to 8 threads                                   
  Compressing objects: 100% (4/4), done.                                    
  Writing objects: 100% (5/5), 590 bytes | 590.00 KiB/s, done.              
  Total 5 (delta 3), reused 0 (delta 0), pack-reused 0 (from 0)             
  remote: Resolving deltas: 100% (3/3), completed with 3 local objects.     
  To https://github.com/anujprabhu01/dplm.git                               
  dca34aa..25555f6  main -> main                                            
  (base) anujprabhu@anujs-mbp dplm % cd /Users/anujprabhu/Scaling-Laws/dplm 
                                                                            
  git add vendor/openfold/setup.py                                          
                                                                            
  git commit -m "Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel          
  compilation"                                                              
  git push                                                                  
  On branch main                                                            
  Your branch is up to date with 'origin/main'.                             
                                                                            
  Changes not staged for commit:                                            
  (use "git add <file>..." to update what will be committed)                
  (use "git restore <file>..." to discard changes in working directory)     
  modified:   configs/datamodule/tcr_2m.yaml                                
  modified:   scripts/README.md                                             
  modified:   scripts/analyze_scaling_results.py                            
  modified:   scripts/run_scaling_experiments.sh                            
  modified:   scripts/run_single_scaling_experiment.sh                      
                                                                            
  Untracked files:                                                          
  (use "git add <file>..." to include in what will be committed)            
  .DS_Store                                                                 
  configs/.DS_Store                                                         
  configs/experiment/.DS_Store                                              
  configs/experiment/structok/.DS_Store                                     
  create_smoke_test_plots.py                                                
  discussion_summary.md                                                     
  src/.DS_Store                                                             
  src/byprot/.DS_Store                                                      
  src/byprot/datamodules/.DS_Store                                          
  src/byprot/models/.DS_Store                                               
  src/byprot/models/dplm/.DS_Store                                          
  src/byprot/tasks/.DS_Store                                                
  src/byprot/utils/.DS_Store                                                
                                                                            
  no changes added to commit (use "git add" and/or "git commit -a")         
  Everything up-to-date                                                     

⏺ The push succeeded (the first command worked - 25555f6 was pushed). Now on the
   wolf server:

  cd ~/data/dplm
  git pull
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

❯ (dplm) adprabh1@wolf:~/data/dplm$ OPENFOLD_CPU_ONLY=1 pip install -e      
  vendor/openfold --no-build-isolation                                      
  Obtaining file:///mnt/disk12/user/adprabh1/dplm/vendor/openfold           
  Checking if build backend supports build_editable ... done                
  Preparing editable metadata (pyproject.toml) ... done                     
  Building wheels for collected packages: openfold                          
  Building editable for openfold (pyproject.toml) ... done                  
  Created wheel for openfold:                                               
  filename=openfold-1.0.1-0.editable-cp39-cp39-linux_x86_64.whl size=7177   
  sha256=009b235716ed67c3ef4922a0378b15445c73756f30b80ed7b1e0e0e4655f3d27   
  Stored in directory: /tmp/pip-ephem-wheel-cache-h083grf4/wheels/27/f9/5   
  7/eecdbafd35389aeed8509fb9c3309b5adc37c54480ae62535d                      
  Successfully built openfold                                               
  Installing collected packages: openfold                                   
  Successfully installed openfold-1.0.1                                     
                                                                            

⏺ OpenFold installed successfully. Now let's verify it works:

  python -c "from openfold.np import residue_constants; print('OpenFold imported
   successfully!')"

  If that works, you now have OpenFold properly installed without the CUDA
  kernel (which you don't need for DPLM sequence-only work).

  Next question: Do you want to:

  1. Keep the current workarounds - The code modifications that made OpenFold
  optional are still in place. Everything should work as-is, but with more code
  changes.
  2. Revert the workarounds - Since OpenFold now installs, you could revert the
  commits that excluded structure-related modules, giving you a cleaner codebase
   closer to the original DPLM repo. This would be better for your paper.

  If you want option 2, we can identify which commits to revert. The relevant
  ones from the git history are:
  - 4a110b9 - Exclude struct_tokenizer task
  - 0a392bd - Exclude structure-related modules (structok, pdb_dataset)
  - 57e5139 - Make OpenFold optional for sequence-only training
  - And a few others

  Would you like to try reverting these to get back to a cleaner codebase?

❯ (dplm) adprabh1@wolf:~/data/dplm$ python -c "from openfold.np import      
  residue_constants; print('OpenFold imported successfully!')"              
  bash: !': event not found                                                 
  (dplm) adprabh1@wolf:~/data/dplm$                                         

⏺ Bash is interpreting ! as history expansion. Try with escaped quotes or
  simpler:

  python -c "from openfold.np import residue_constants; print('OpenFold works')"

  Or:

  python -c 'from openfold.np import residue_constants; print("OpenFold works")'

❯ (dplm) adprabh1@wolf:~/data/dplm$   python -c "from openfold.np import    
  residue_constants; print('OpenFold works')"                               
                                                                            
  OpenFold works                                                            
                                                                            

⏺ OpenFold is working. Now you have two options:

  Option 1: Keep current state, proceed with experiments
  - Faster - start running experiments now
  - Code has workarounds but everything works

  Option 2: Revert workarounds for cleaner codebase
  - Better for paper/reproducibility
  - Takes some time to revert and test
  - Closer to original DPLM repo

  My recommendation: Option 2 - since you mentioned wanting minimal code
  modifications for your paper. The workarounds were only needed because
  OpenFold wouldn't install. Now that it does, you can revert them.

  To see what would be reverted:

  git log --oneline 8a2e15e..HEAD | grep -i "openfold\|exclude\|optional"

  This will show the commits that added workarounds. Want me to help you revert
  them systematically, or would you prefer to proceed with the current state and
   focus on running experiments?

❯ what if I keep this repo as is and clone the DPLM repo again to restart?  
  if something goes wrong, I can fall back to this repo. but, I think       
  minimum code modifications is better for readability, reproducability.    

⏺ That's a smart approach - clean slate with a safety net. Here's the plan:

  On wolf server:

  cd ~/data

  # Rename current repo as backup
  mv dplm dplm_backup

  # Clone fresh from original DPLM (not your fork)
  git clone https://github.com/bytedance/dplm.git

  cd dplm

  # Apply ONLY the one fix needed - the OPENFOLD_CPU_ONLY env var
  # (I can give you the exact edit)

  # Install dependencies
  conda activate dplm
  pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url
  https://download.pytorch.org/whl/cu121
  pip install -e .
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

  # Copy over your data
  cp -r ../dplm_backup/data-bin ./
  cp -r ../dplm_backup/logs ./  # if you want to keep old checkpoints

  The only code change needed is adding the 4 lines to vendor/openfold/setup.py
  that we just created.

  Want to proceed with this? I can also help you set up your own fork of the
  fresh repo so you can track your minimal changes for the paper.

❯ /export 
  ⎿  Conversation exported to:
     2026-01-27-is-there-a-subdirectory-called-logs-in-this-direct.txt

❯ yes, help me set up my own fork of the fresh repo so I can track the      
  minimal changes for the paper                                             

⏺ Here's the step-by-step plan:

  Step 1: Create a fresh fork on GitHub

  1. Go to https://github.com/bytedance/dplm
  2. Click "Fork" in the top right
  3. Since you already have anujprabhu01/dplm, GitHub may ask you to rename it.
  You could:
    - Delete your existing fork (risky - loses history)
    - Or rename the new fork to something like dplm-scaling

  Better option: Keep your existing fork but reset it to match upstream:

  # On your local machine
  cd /Users/anujprabhu/Scaling-Laws

  # Clone fresh from original
  git clone https://github.com/bytedance/dplm.git dplm_clean
  cd dplm_clean

  # Change remote to point to YOUR fork
  git remote remove origin
  git remote add origin https://github.com/anujprabhu01/dplm.git
  git remote add upstream https://github.com/bytedance/dplm.git

  # Force push to reset your fork to original state
  git push origin main --force

  Step 2: Apply the single OpenFold fix

  I'll create the minimal patch file for you. Want me to proceed with this
  approach?

  Once set up, your fork will have:
  - Clean history starting from original DPLM
  - Only 1 commit for the OpenFold CPU fix
  - Any future changes clearly tracked

❯ I can't create 2 forks of the same repo under same ownership? 

⏺ Correct - GitHub only allows one fork per repo per account.

  Your options:

  Option A: Reset your existing fork (recommended)
  Your backup is safe on wolf at dplm_backup. Force-push your fork to match
  upstream:

  # Local machine
  cd /Users/anujprabhu/Scaling-Laws
  mv dplm dplm_old_local  # backup local copy too

  git clone https://github.com/bytedance/dplm.git dplm
  cd dplm

  # Point to your fork
  git remote set-url origin https://github.com/anujprabhu01/dplm.git
  git remote add upstream https://github.com/bytedance/dplm.git

  # Reset your fork to original
  git push origin main --force

  Option B: Create a new repo (not a fork)
  Create a new repo like dplm-scaling on GitHub, then:

  git clone https://github.com/bytedance/dplm.git dplm-scaling
  cd dplm-scaling
  git remote set-url origin https://github.com/anujprabhu01/dplm-scaling.git
  git push -u origin main

  Option A is cleaner since it keeps the connection to upstream DPLM for
  potential future PRs or syncing. Your backup on wolf preserves all your old
  work.

  Which do you prefer?

❯ why is this so complex? why can't I just keep my current repo and create  
  a new fork                                                                

⏺ You're right to push back - it's a GitHub limitation, not a git one.

  Why GitHub doesn't allow it: They use forks for tracking contributions back to
   the original repo. One account = one fork relationship per repo.

  Simplest path forward:

  Just create a new repo (not a fork) called dplm-scaling:

  1. Go to GitHub → New repository → name it dplm-scaling
  2. Then:

  cd /Users/anujprabhu/Scaling-Laws
  git clone https://github.com/bytedance/dplm.git dplm-scaling
  cd dplm-scaling
  git remote set-url origin https://github.com/anujprabhu01/dplm-scaling.git
  git push -u origin main

  That's it. You now have:
  - anujprabhu01/dplm - your old work (backup)
  - anujprabhu01/dplm-scaling - fresh start with minimal changes

  The only downside of "not a fork" is you can't easily open PRs to the original
   DPLM repo - but you probably don't need that for a scaling laws paper anyway.

❯ what if origin has some update in this future? like new commits to main? 

⏺ You can still sync with upstream - just add it as a remote:

  cd dplm-scaling
  git remote add upstream https://github.com/bytedance/dplm.git

  # Whenever you want to pull updates from original DPLM:
  git fetch upstream
  git merge upstream/main
  # or
  git rebase upstream/main

  "Fork" on GitHub is just a UI convenience - underneath it's all just git
  remotes. Your dplm-scaling repo can track bytedance/dplm exactly the same way.

  So you'll have:
  - origin → your anujprabhu01/dplm-scaling (push your work here)
  - upstream → original bytedance/dplm (pull updates from here)

  This is actually the standard workflow even for GitHub forks.

❯ the better option is to just revert the commits no? 

⏺ Yes, you're right - that's simpler. You already have:
  - Your fork with history
  - The OPENFOLD_CPU_ONLY fix already committed
  - OpenFold working on wolf

  Just revert the workaround commits and you're done. Let me find them:

  git log --oneline --all | head -30

  Or I can look at what was changed. The commits to revert are the ones that
  made OpenFold "optional" by excluding modules. Let me check your git history:

⏺ Bash(git log --oneline | head -40)
  ⎿ 25555f6 Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel compilation
    dca34aa Fix: Use uniref50_hf datamodule instead of custom tcr_2m config
    95c6151 Fix: Adjust min_len for TCR sequences (5 instead of 50) and han
    … +40 lines (ctrl+o to expand)



⏺ Now I can see the structure. The commits to revert (OpenFold workarounds) are:

  57e5139 Make OpenFold optional for sequence-only training
  0e2df1e Fix OpenFold CUDA compilation for CUDA 12.0+
  83fa214 Fix bare_metal_major undefined error in OpenFold setup.py
  7793777 Make torch_scatter optional for sequence-only training
  763fb6d Fix SHAPE_SCHEMA initialization when OpenFold unavailable
  4f00fe0 Fix GROUP_IDX initialization when OpenFold unavailable
  0a392bd Exclude structure-related modules (structok, pdb_dataset)
  4a110b9 Exclude struct_tokenizer task that requires OpenFold

  Keep these (useful for your experiments):
  - 25555f6 - OPENFOLD_CPU_ONLY (needed!)
  - Scaling configs, TCR fixes, smoke test scripts, CLAUDE.md, etc.

  Simplest approach - restore specific files to original state:

  # Restore the files that had workarounds to their original versions
  git checkout 8a2e15e -- src/byprot/datamodules/__init__.py
  git checkout 8a2e15e -- src/byprot/models/__init__.py
  git checkout 8a2e15e -- src/byprot/tasks/__init__.py
  git checkout 8a2e15e -- src/byprot/datamodules/pdb_dataset/all_atom.py
  git checkout 8a2e15e -- src/byprot/datamodules/pdb_dataset/pdb_datamodule.py
  git checkout 8a2e15e -- src/byprot/datamodules/pdb_dataset/utils.py

  # Commit the reversion
  git commit -m "Revert OpenFold workarounds - now using OPENFOLD_CPU_ONLY 
  instead"

  This keeps your useful additions (configs, scripts) while reverting only the
  workaround code. Want to proceed?

✻ Brewed for 31s

❯ I want to start fresh keeping my current repo as backup. 

⏺ Simple and clean. Here's the plan:

  1. Create new repo on GitHub
  Go to https://github.com/new → name it dplm-scaling → Create (empty, no
  README)

  2. On your local machine:
  cd /Users/anujprabhu/Scaling-Laws

  # Clone original DPLM
  git clone https://github.com/bytedance/dplm.git dplm-scaling
  cd dplm-scaling

  # Set up remotes
  git remote set-url origin https://github.com/anujprabhu01/dplm-scaling.git
  git remote add upstream https://github.com/bytedance/dplm.git

  # Push to your new repo
  git push -u origin main

  3. Apply the one fix (I'll do this for you after you run the above)

  4. On wolf server:
  cd ~/data
  mv dplm dplm_backup
  git clone https://github.com/anujprabhu01/dplm-scaling.git dplm
  cd dplm
  # Install with OPENFOLD_CPU_ONLY=1

  Let me know once you've created the repo and cloned it locally.

❯ why name it dplm_backup? why not just leave the name as is on wolf and    
  cone as dplm-scaling                                                      

⏺ You're right, that's simpler:

  On wolf:
  cd ~/data
  git clone https://github.com/anujprabhu01/dplm-scaling.git
  cd dplm-scaling
  # Install here

  Now you have:
  - ~/data/dplm - old work (backup)
  - ~/data/dplm-scaling - fresh start

  Go ahead and create the dplm-scaling repo on GitHub, then run the local clone
  commands.

❯ @README.md the "Installation" section of the README tells me to do this: 
  ⎿  Read README.md (642 lines)
  ⎿  Interrupted · What should Claude do instead?

❯ @README.md the "Installation" section of the README tells me to do this:  
                                                                            
  # clone project                                                           
  git clone --recursive https://url/to/this/repo/dplm.git                   
  cd dplm                                                                   
                                                                            
  # create conda virtual environment                                        
  env_name=dplm                                                             
                                                                            
  conda create -n ${env_name} python=3.9 pip                                
  conda activate ${env_name}                                                
                                                                            
  # automatically install everything else                                   
  bash scripts/install.sh                                                   
                                                                            
  how to proceed                                                            
  ⎿  Read README.md (642 lines)

⏺ Here's the full process:

  Step 1: Create dplm-scaling repo on GitHub (empty, no README)

  Step 2: Local machine
  cd /Users/anujprabhu/Scaling-Laws

  # Clone original DPLM
  git clone --recursive https://github.com/bytedance/dplm.git dplm-scaling
  cd dplm-scaling

  # Set up remotes
  git remote set-url origin https://github.com/anujprabhu01/dplm-scaling.git
  git remote add upstream https://github.com/bytedance/dplm.git

  # Push to your repo
  git push -u origin main

  Step 3: I'll apply the OPENFOLD_CPU_ONLY fix (tell me when ready)

  Step 4: On wolf server
  cd ~/data

  # Clone your new repo
  git clone --recursive https://github.com/anujprabhu01/dplm-scaling.git

  cd dplm-scaling

  # Create fresh conda env (or reuse existing one)
  conda activate dplm

  # Install
  bash scripts/install.sh

  # If install.sh fails on OpenFold, run:
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

  Let me know once you've created the GitHub repo and done the local clone -
  I'll then add the one-line fix to vendor/openfold/setup.py.

❯ I didn't do recursive.                                                    
                                                                            
  ❯ I want to start fresh keeping my current repo as backup.                
                                                                            
                                                                            
                                                                            
  ⏺ Simple and clean. Here's the plan:                                      
                                                                            
                                                                            
                                                                            
  1. Create new repo on GitHub                                              
                                                                            
  Go to https://github.com/new → name it dplm-scaling → Create (empty, no   
  README)                                                                   
                                                                            
                                                                            
  2. On your local machine:                                                 
                                                                            
  cd /Users/anujprabhu/Scaling-Laws                                         
                                                                            
                                                                            
                                                                            
  # Clone original DPLM                                                     
                                                                            
  git clone https://github.com/bytedance/dplm.git dplm-scaling              
                                                                            
  cd dplm-scaling                                                           
                                                                            
                                                                            
                                                                            
  # Set up remotes                                                          
                                                                            
  git remote set-url origin                                                 
  https://github.com/anujprabhu01/dplm-scaling.git                          
                                                                            
  git remote add upstream https://github.com/bytedance/dplm.git             
                                                                            
                                                                            
                                                                            
  # Push to your new repo                                                   
                                                                            
  git push -u origin main                                                   
                                                                            
                                                                            
                                                                            
  3. Apply the one fix (I'll do this for you after you run the above)       
                                                                            
                                                                            
                                                                            
  4. On wolf server:                                                        
                                                                            
  cd ~/data                                                                 
                                                                            
  mv dplm dplm_backup                                                       
                                                                            
  git clone https://github.com/anujprabhu01/dplm-scaling.git dplm           
                                                                            
  cd dplm                                                                   
                                                                            
  # Install with OPENFOLD_CPU_ONLY=1                                        
                                                                            
                                                                            
                                                                            
  Let me know once you've created the repo and cloned it locally.           
                                                                            
                                                                            
                                                                            
  ❯ why name it dplm_backup? why not just leave the name as is on wolf and  
  cone as dplm-scaling                                                      
                                                                            
                                                                            
  ⏺ You're right, that's simpler:                                           
                                                                            
                                                                            
                                                                            
  On wolf:                                                                  
                                                                            
  cd ~/data                                                                 
                                                                            
  git clone https://github.com/anujprabhu01/dplm-scaling.git                
                                                                            
  cd dplm-scaling                                                           
                                                                            
  # Install here                                                            
                                                                            
                                                                            
                                                                            
  Now you have:                                                             
                                                                            
  - ~/data/dplm - old work (backup)                                         
                                                                            
  - ~/data/dplm-scaling - fresh start                                       
                                                                            
                                                                            
                                                                            
  Go ahead and create the dplm-scaling repo on GitHub, then run the local   
  clone commands.                                                           

⏺ That's probably fine. The --recursive flag is for git submodules, but looking
  at the repo, vendor/openfold is directly included (not a submodule).

  You can verify with:
  cd /Users/anujprabhu/Scaling-Laws/dplm-scaling
  ls vendor/openfold

  If files are there, you're good. If it's empty, run:
  git submodule update --init --recursive

  Did you already complete these steps?
  1. Created dplm-scaling repo on GitHub
  2. Cloned locally and set up remotes
  3. Pushed to origin

  If yes, let me know and I'll apply the OpenFold fix.

❯   git submodule update --init --recursive                                 
                                                                            
  should I run this regardless?                                             

⏺ Yes, run it. It's harmless if there are no submodules (does nothing), and
  fixes things if there are. Quick and safe.

  git submodule update --init --recursive

❯ # create conda virtual environment                                        
  env_name=dplm                                                             
                                                                            
  conda create -n ${env_name} python=3.9 pip                                
  conda activate ${env_name}                                                
                                                                            
  # automatically install everything else                                   
  bash scripts/install.sh                                                   
                                                                            
  should we follow with these steps?                                        

⏺ For your workflow (edit locally → push → pull on wolf → run on wolf), you
  probably don't need a full local install.

  Locally - just need the repo for editing:
  1. Apply the OpenFold fix (I'll do this now)
  2. Push to GitHub

  On wolf - do the full install there since that's where you run experiments.

  Let me apply the fix now:

⏺ Read 1 file (ctrl+o to expand)

⏺ Update(~/Scaling-Laws/dplm-scaling/vendor/openfold/setup.py)
  ⎿  (No changes)

❯ can you explain what this code does? 

⏺ This setup.py builds and installs OpenFold as a Python package. Here's what
  each part does:

  Lines 19-22: Import torch and CUDA extension builders. Also imports a helper
  to detect GPU compute capability.

  Lines 25-38: Define compiler flags for CUDA compilation (C++ standard,
  optimizations, etc.)

  Lines 40-52: get_cuda_bare_metal_version() - Checks if CUDA is available:
  - If no CUDA → returns -1 (triggers CPU-only build)
  - If CUDA exists → runs nvcc -V to get CUDA version

  Lines 54-67: Determine which GPU architectures to compile for:
  - Default: old GPUs (K80, Titan X, GTX 1000-series)
  - If CUDA 11+ → add sm_80 (A100)
  - If GPU detected → use only that GPU's compute capability

  Lines 69-76: Build the -gencode flags for nvcc (tells it which GPU
  architectures to target)

  Lines 78-110: The key decision:
  - If CUDA found (bare_metal_major != -1): Build CUDAExtension with the .cu
  kernel file
  - If no CUDA (bare_metal_major == -1): Build CppExtension with a CPU stub
  instead

  Lines 112-134: Standard setuptools configuration (package name, version, files
   to include, etc.)

  ---
  The fix I want to add (lines 40-43): Check for OPENFOLD_CPU_ONLY=1 environment
   variable to force the CPU stub path, bypassing the CUDA kernel compilation
  that fails on your system.

❯ go ahead and apply the fix 

⏺ Update(~/Scaling-Laws/dplm-scaling/vendor/openfold/setup.py)
  ⎿  Added 4 lines
      38  ]
      39  
      40  def get_cuda_bare_metal_version(cuda_dir):
      41 +    # Allow forcing CPU-only build via environment variable    
      42 +    if os.environ.get("OPENFOLD_CPU_ONLY", "0") == "1":        
      43 +        print("OPENFOLD_CPU_ONLY=1 set, installing CPU-only ver
         +sion")                                                         
      44 +        return None, -1, 0                                     
      45      if cuda_dir==None or torch.version.cuda==None:
      46          print("CUDA is not found, cpu version is installed")
      47          return None, -1, 0

⏺ Done. Now commit and push:

  cd /Users/anujprabhu/Scaling-Laws/dplm-scaling
  git add vendor/openfold/setup.py
  git commit -m "Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel compilation"
  git push

  Then on wolf:

  cd ~/data
  git clone https://github.com/anujprabhu01/dplm-scaling.git
  cd dplm-scaling
  conda activate dplm
  bash scripts/install.sh
  # If OpenFold fails during install.sh, run:
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

❯ I already cloned on wold 

⏺ Got it. Then:

  Locally:
  cd /Users/anujprabhu/Scaling-Laws/dplm-scaling
  git add vendor/openfold/setup.py
  git commit -m "Add OPENFOLD_CPU_ONLY env var to skip CUDA kernel compilation"
  git push

  On wolf:
  cd ~/data/dplm-scaling
  git pull
  conda activate dplm
  bash scripts/install.sh

  If OpenFold fails during install.sh:
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

❯ but does the conda env dplm exist? 

⏺ Yes, it exists from your previous work on wolf. You can reuse it since the
  dependencies are the same.

  Check with:
  conda env list

  You should see dplm in the list. Then just:
  conda activate dplm
  bash scripts/install.sh

  If you want a completely fresh environment instead, create a new one:
  conda create -n dplm-scaling python=3.9 pip -y
  conda activate dplm-scaling
  bash scripts/install.sh

  Either works - reusing dplm is faster.

❯ what is the -y flag 

⏺ The -y flag means "yes to all prompts" - it auto-confirms without asking you
  to type y to proceed.

  Without -y:
  Proceed ([y]/n)? _   # waits for you to type y

  With -y:
  # just runs immediately

❯ (base) anujprabhu@Anujs-MacBook-Pro ~ % ssh adprabh1@10.173.14.193        
  adprabh1@10.173.14.193's password:                                        
  (base) adprabh1@wolf:~$ cd data/dplm/                                     
  (base) adprabh1@wolf:~/data/dplm$ ls                                      
  analysis  CLAUDE.md  data-bin  generate_dplm2.py  generation-results      
  logs                  README.md         run      setup.cfg  src           
  train.py                                                                  
  assets    configs    env.yml   generate_dplm.py   LICENSE                 
  openfold_install.log  requirements.txt  scripts  setup.py   test.py       
  vendor                                                                    
  (base) adprabh1@wolf:~/data/dplm$ conda activate dplm                     
  (dplm) adprabh1@wolf:~/data/dplm$ python -c "from openfold.data import    
  data_transforms; from openfold.np import residue_constants; from          
  openfold.utils import rigid_utils as ru"                                  
  (dplm) adprabh1@wolf:~/data/dplm$ cd ..                                   
  (dplm) adprabh1@wolf:~/data$ ls                                           
  dplm  miniconda3  tcr_dataset                                             
  (dplm) adprabh1@wolf:~/data$ ls ./dplm/                                   
  analysis  CLAUDE.md  data-bin  generate_dplm2.py  generation-results      
  logs                  README.md         run      setup.cfg  src           
  train.py                                                                  
  assets    configs    env.yml   generate_dplm.py   LICENSE                 
  openfold_install.log  requirements.txt  scripts  setup.py   test.py       
  vendor                                                                    
  (dplm) adprabh1@wolf:~/data$ git clone                                    
  https://github.com/anujprabhu01/dplm-scaling.git                          
  Cloning into 'dplm-scaling'...                                            
  remote: Enumerating objects: 893, done.                                   
  remote: Counting objects: 100% (893/893), done.                           
  remote: Compressing objects: 100% (427/427), done.                        
  remote: Total 893 (delta 434), reused 893 (delta 434), pack-reused 0      
  (from 0)                                                                  
  Receiving objects: 100% (893/893), 13.07 MiB | 13.40 MiB/s, done.         
  Resolving deltas: 100% (434/434), done.                                   
  (dplm) adprabh1@wolf:~/data$ git remote -v                                
  fatal: not a git repository (or any parent up to mount point /mnt)        
  Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not      
  set).                                                                     
  (dplm) adprabh1@wolf:~/data$ cd dplm-scaling/                             
  (dplm) adprabh1@wolf:~/data/dplm-scaling$ git remote -v                   
  origin    https://github.com/anujprabhu01/dplm-scaling.git (fetch)        
  origin    https://github.com/anujprabhu01/dplm-scaling.git (push)         
  (dplm) adprabh1@wolf:~/data/dplm-scaling$ cd                              
  /Users/anujprabhu/Scaling-Laws/dplm-scaling                               
  bash: cd: /Users/anujprabhu/Scaling-Laws/dplm-scaling: No such file or    
  directory                                                                 
  (dplm) adprabh1@wolf:~/data/dplm-scaling$   git submodule update --init   
  --recursive                                                               
                                                                            
  (dplm) adprabh1@wolf:~/data/dplm-scaling$ conda create -n dplm-scaling    
  python=3.9 pip -y                                                         
                                                                            
  conda activate dplm-scaling                                               
                                                                            
  bash scripts/install.sh                                                   
  2 channel Terms of Service accepted                                       
  Retrieving notices: done                                                  
  Channels:                                                                 
  - defaults                                                                
  Platform: linux-64                                                        
  Collecting package metadata (repodata.json): done                         
  Solving environment: done                                                 
                                                                            
  ## Package Plan ##                                                        
                                                                            
  environment location: /home/adprabh1/data/miniconda3/envs/dplm-scaling    
                                                                            
  added / updated specs:                                                    
  - pip                                                                     
  - python=3.9                                                              
                                                                            
                                                                            
  The following packages will be downloaded:                                
                                                                            
  package                    |            build                             
  ---------------------------|-----------------                             
  openssl-3.0.19             |       h1b28b03_0         4.4 MB              
  sqlite-3.51.1              |       he0a8d7e_0         1.2 MB              
  tzdata-2025c               |       he532380_0         118 KB              
  ------------------------------------------------------------              
  Total:         5.7 MB                                                     
                                                                            
  The following NEW packages will be INSTALLED:                             
                                                                            
  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main             
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu            
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6             
  ca-certificates                                                           
  pkgs/main/linux-64::ca-certificates-2025.12.2-h06a4308_0                  
  expat              pkgs/main/linux-64::expat-2.7.3-h7354ed3_4             
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.44-h153f514_2   
                                                                            
  libexpat           pkgs/main/linux-64::libexpat-2.7.3-h7354ed3_4          
  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1            
  libgcc             pkgs/main/linux-64::libgcc-15.2.0-h69a1729_7           
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-15.2.0-h166f726_7        
  libgomp            pkgs/main/linux-64::libgomp-15.2.0-h4751f2c_7          
  libnsl             pkgs/main/linux-64::libnsl-2.0.0-h5eee18b_0            
  libstdcxx          pkgs/main/linux-64::libstdcxx-15.2.0-h39759b7_7        
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-15.2.0-hc03a8fd_7     
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0          
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0           
  libzlib            pkgs/main/linux-64::libzlib-1.3.1-hb25bd0a_0           
  ncurses            pkgs/main/linux-64::ncurses-6.5-h7934f7d_0             
  openssl            pkgs/main/linux-64::openssl-3.0.19-h1b28b03_0          
  pip                pkgs/main/noarch::pip-25.3-pyhc872135_0                
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1       
  python             pkgs/main/linux-64::python-3.9.25-h0dcde21_1           
  readline           pkgs/main/linux-64::readline-8.3-hc2a1206_0            
  setuptools         pkgs/main/linux-64::setuptools-80.9.0-py39h06a4308_0   
                                                                            
  sqlite             pkgs/main/linux-64::sqlite-3.51.1-he0a8d7e_0           
  tk                 pkgs/main/linux-64::tk-8.6.15-h54e0aa7_0               
  tzdata             pkgs/main/noarch::tzdata-2025c-he532380_0              
  wheel              pkgs/main/linux-64::wheel-0.45.1-py39h06a4308_0        
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1      
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0      
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0     
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1   
                                                                            
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1                
  zlib               pkgs/main/linux-64::zlib-1.3.1-hb25bd0a_0              
                                                                            
                                                                            
                                                                            
  Downloading and Extracting Packages:                                      
                                                                            
                                                                            
                                                                            
  Preparing transaction: done                                               
                                                                            
                                                                            
  Verifying transaction: done                                               
                                                                            
                                                                            
  Executing transaction: done                                               
  #                                                                         
  # To activate this environment, use                                       
  #                                                                         
  #     $ conda activate dplm-scaling                                       
  #                                                                         
  # To deactivate an active environment, use                                
  #                                                                         
  #     $ conda deactivate                                                  
                                                                            
  Looking in indexes: https://download.pytorch.org/whl/cu121                
  Collecting torch==2.2.0                                                   
  Using cached https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121   
  -cp39-cp39-linux_x86_64.whl (757.3 MB)                                    
  Collecting torchvision==0.17.0                                            
  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.17.0%   
  2Bcu121-cp39-cp39-linux_x86_64.whl (7.0 MB)                               
  Collecting torchaudio==2.2.0                                              
  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2B   
  cu121-cp39-cp39-linux_x86_64.whl (3.4 MB)                                 
  Collecting filelock (from torch==2.2.0)                                   
  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)           
  Collecting typing-extensions>=4.8.0 (from torch==2.2.0)                   
  Using cached https://download.pytorch.org/whl/typing_extensions-4.15.0-   
  py3-none-any.whl.metadata (3.3 kB)                                        
  Collecting sympy (from torch==2.2.0)                                      
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)               
  Collecting networkx (from torch==2.2.0)                                   
  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)            
  Collecting jinja2 (from torch==2.2.0)                                     
  Using cached                                                              
  https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata   
  (2.9 kB)                                                                  
  Collecting fsspec (from torch==2.2.0)                                     
  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)           
  Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)           
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_c   
  u12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)                     
  Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)         
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime   
  _cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)                    
  Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)           
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_c   
  u12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)                     
  Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)                
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8   
  .9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)                         
  Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)               
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-   
  12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)                        
  Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)               
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-1   
  1.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)                        
  Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)             
  Using cached https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-   
  10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)                       
  Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)           
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu1   
  2-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)                    
  Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)           
  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu1   
  2-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)                    
  Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)                   
  Using cached https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.   
  19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)                            
  Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)                 
  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12   
  .1.105-py3-none-manylinux1_x86_64.whl (99 kB)                             
  Collecting triton==2.2.0 (from torch==2.2.0)                              
  Using cached https://download.pytorch.org/whl/triton-2.2.0-cp39-cp39-ma   
  nylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)                   
  Collecting numpy (from torchvision==0.17.0)                               
  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_   
  x86_64.whl.metadata (60 kB)                                               
  Collecting requests (from torchvision==0.17.0)                            
  Using cached                                                              
  https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62 kB) 
  Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.17.0)               
  Using cached pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_   
  28_x86_64.whl.metadata (9.0 kB)                                           
  Collecting nvidia-nvjitlink-cu12 (from                                    
  nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)                           
  Using cached https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.   
  9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata     
  (1.7 kB)                                                                  
  Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0)                    
  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp39-cp3   
  9-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)                  
  Collecting charset-normalizer<3,>=2 (from requests->torchvision==0.17.0)  
  Using cached https://download.pytorch.org/whl/charset_normalizer-2.1.1-   
  py3-none-any.whl (39 kB)                                                  
  Collecting idna<4,>=2.5 (from requests->torchvision==0.17.0)              
  Using cached https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl   
  (61 kB)                                                                   
  Collecting urllib3<1.27,>=1.21.1 (from requests->torchvision==0.17.0)     
  Using cached                                                              
  https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl     
  (140 kB)                                                                  
  Collecting certifi>=2017.4.17 (from requests->torchvision==0.17.0)        
  Using cached                                                              
  https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155  
  kB)                                                                       
  Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0)                  
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)              
  Using cached                                                              
  pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl   
  (6.6 MB)                                                                  
  Using cached https://download.pytorch.org/whl/typing_extensions-4.15.0-py 
  3-none-any.whl (44 kB)                                                    
  Using cached filelock-3.19.1-py3-none-any.whl (15 kB)                     
  Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)                   
  Using cached                                                              
  https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (134 kB)   
  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)                     
  Using cached                                                              
  numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl      
  (19.5 MB)                                                                 
  Using cached https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9. 
  86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)      
  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)                       
  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)                       
  Installing collected packages: mpmath, urllib3, typing-extensions, sympy, 
  pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12,        
  nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12,          
  nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12,       
  numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer,  
  certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12,       
  jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio              
  Successfully installed MarkupSafe-2.1.5 certifi-2022.12.7                 
  charset-normalizer-2.1.1 filelock-3.19.1 fsspec-2025.10.0 idna-3.4        
  jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 numpy-2.0.2                      
  nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105               
  nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105         
  nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54                    
  nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107             
  nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3                   
  nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pillow-11.3.0     
  requests-2.28.1 sympy-1.14.0 torch-2.2.0+cu121 torchaudio-2.2.0+cu121     
  torchvision-0.17.0+cu121 triton-2.2.0 typing-extensions-4.15.0            
  urllib3-1.26.13                                                           
  Obtaining file:///mnt/disk12/user/adprabh1/dplm-scaling                   
  Installing build dependencies ... done                                    
  Checking if build backend supports build_editable ... done                
  Getting requirements to build editable ... done                           
  Preparing editable metadata (pyproject.toml) ... done                     
  Collecting pytorch_lightning==2.2.0 (from ByProt==1.0.0)                  
  Using cached pytorch_lightning-2.2.0-py3-none-any.whl.metadata (21 kB)    
  Collecting lightning==2.2.0 (from ByProt==1.0.0)                          
  Using cached lightning-2.2.0-py3-none-any.whl.metadata (56 kB)            
  Collecting torchmetrics (from ByProt==1.0.0)                              
  Using cached torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)         
  Collecting torch_geometric (from ByProt==1.0.0)                           
  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)      
  Collecting torchtext==0.17.0 (from ByProt==1.0.0)                         
  Using cached torchtext-0.17.0-cp39-cp39-manylinux1_x86_64.whl.metadata    
  (7.6 kB)                                                                  
  Collecting hydra-core==1.2.0 (from ByProt==1.0.0)                         
  Using cached hydra_core-1.2.0-py3-none-any.whl.metadata (4.0 kB)          
  Collecting hydra-colorlog==1.2.0 (from ByProt==1.0.0)                     
  Using cached hydra_colorlog-1.2.0-py3-none-any.whl.metadata (949 bytes)   
  Collecting hydra-optuna-sweeper==1.2.0 (from ByProt==1.0.0)               
  Using cached hydra_optuna_sweeper-1.2.0-py3-none-any.whl.metadata (1.0    
  kB)                                                                       
  Collecting tensorboard (from ByProt==1.0.0)                               
  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)        
  Collecting pyrootutils (from ByProt==1.0.0)                               
  Using cached pyrootutils-1.0.4-py3-none-any.whl.metadata (4.0 kB)         
  Collecting pre-commit (from ByProt==1.0.0)                                
  Using cached pre_commit-4.3.0-py2.py3-none-any.whl.metadata (1.2 kB)      
  Collecting black (from ByProt==1.0.0)                                     
  Using cached black-25.11.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_1   
  7_x86_64.manylinux_2_28_x86_64.whl.metadata (85 kB)                       
  Collecting isort (from ByProt==1.0.0)                                     
  Using cached isort-6.1.0-py3-none-any.whl.metadata (12 kB)                
  Collecting flake8 (from ByProt==1.0.0)                                    
  Using cached flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)          
  Collecting nbstripout (from ByProt==1.0.0)                                
  Using cached nbstripout-0.8.2-py2.py3-none-any.whl.metadata (21 kB)       
  Collecting python-dotenv (from ByProt==1.0.0)                             
  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)        
  Collecting rich (from ByProt==1.0.0)                                      
  Downloading rich-14.3.1-py3-none-any.whl.metadata (18 kB)                 
  Collecting pytest (from ByProt==1.0.0)                                    
  Using cached pytest-8.4.2-py3-none-any.whl.metadata (7.7 kB)              
  Collecting sh (from ByProt==1.0.0)                                        
  Using cached sh-2.2.2-py3-none-any.whl.metadata (3.7 kB)                  
  Collecting pudb (from ByProt==1.0.0)                                      
  Using cached pudb-2025.1.5-py3-none-any.whl.metadata (4.4 kB)             
  Collecting biopython==1.79 (from ByProt==1.0.0)                           
  Using cached biopython-1.79-cp39-cp39-manylinux_2_17_x86_64.manylinux20   
  14_x86_64.whl.metadata (12 kB)                                            
  Collecting biotite (from ByProt==1.0.0)                                   
  Using cached biotite-0.40.0-cp39-cp39-linux_x86_64.whl                    
  Collecting datasets==2.20.0 (from ByProt==1.0.0)                          
  Using cached datasets-2.20.0-py3-none-any.whl.metadata (19 kB)            
  Collecting deepspeed==0.14.4 (from ByProt==1.0.0)                         
  Using cached deepspeed-0.14.4-py3-none-any.whl                            
  Collecting dm-tree (from ByProt==1.0.0)                                   
  Using cached dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux201   
  4_x86_64.whl.metadata (1.9 kB)                                            
  Collecting e3nn (from ByProt==1.0.0)                                      
  Using cached e3nn-0.5.9-py3-none-any.whl.metadata (6.0 kB)                
  Collecting einops (from ByProt==1.0.0)                                    
  Downloading einops-0.8.2-py3-none-any.whl.metadata (13 kB)                
  Collecting debugpy (from ByProt==1.0.0)                                   
  Downloading debugpy-1.8.20-cp39-cp39-manylinux_2_34_x86_64.whl.metadata   
  (1.4 kB)                                                                  
  Collecting fair-esm (from ByProt==1.0.0)                                  
  Using cached fair_esm-2.0.0-py3-none-any.whl.metadata (37 kB)             
  Collecting lmdb (from ByProt==1.0.0)                                      
  Using cached lmdb-1.7.5-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x   
  86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)                         
  Collecting matplotlib (from ByProt==1.0.0)                                
  Using cached matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux   
  2014_x86_64.whl.metadata (11 kB)                                          
  Collecting mdtraj (from ByProt==1.0.0)                                    
  Using cached mdtraj-1.10.3-cp39-cp39-linux_x86_64.whl                     
  Collecting ml-collections (from ByProt==1.0.0)                            
  Using cached ml_collections-0.1.1-py3-none-any.whl                        
  Collecting modelcif (from ByProt==1.0.0)                                  
  Using cached modelcif-1.6-py3-none-any.whl.metadata (3.4 kB)              
  Collecting opt_einsum (from ByProt==1.0.0)                                
  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)          
  Collecting pandas (from ByProt==1.0.0)                                    
  Using cached pandas-2.3.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_2   
  8_x86_64.whl.metadata (91 kB)                                             
  Collecting peft==0.11.1 (from ByProt==1.0.0)                              
  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)                
  Collecting scikit-learn (from ByProt==1.0.0)                              
  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylin   
  ux2014_x86_64.whl.metadata (18 kB)                                        
  Collecting seaborn (from ByProt==1.0.0)                                   
  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)            
  Requirement already satisfied: sympy in /mnt/disk12/user/adprabh1/minicon 
  da3/envs/dplm-scaling/lib/python3.9/site-packages (from ByProt==1.0.0)    
  (1.14.0)                                                                  
  Collecting tmtools (from ByProt==1.0.0)                                   
  Using cached tmtools-0.3.0-cp39-cp39-linux_x86_64.whl                     
  Collecting transformers==4.39.2 (from ByProt==1.0.0)                      
  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)       
  Collecting MDAnalysis (from ByProt==1.0.0)                                
  Using cached MDAnalysis-2.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux   
  2014_x86_64.whl.metadata (46 kB)                                          
  Requirement already satisfied: numpy in /mnt/disk12/user/adprabh1/minicon 
  da3/envs/dplm-scaling/lib/python3.9/site-packages (from                   
  biopython==1.79->ByProt==1.0.0) (2.0.2)                                   
  Requirement already satisfied: filelock in /mnt/disk12/user/adprabh1/mini 
  conda3/envs/dplm-scaling/lib/python3.9/site-packages (from                
  datasets==2.20.0->ByProt==1.0.0) (3.19.1)                                 
  Collecting pyarrow>=15.0.0 (from datasets==2.20.0->ByProt==1.0.0)         
  Using cached                                                              
  pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)      
  Collecting pyarrow-hotfix (from datasets==2.20.0->ByProt==1.0.0)          
  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)        
  Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->ByProt==1.0.0)      
  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)                 
  Collecting requests>=2.32.2 (from datasets==2.20.0->ByProt==1.0.0)        
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)           
  Collecting tqdm>=4.66.3 (from datasets==2.20.0->ByProt==1.0.0)            
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)                
  Collecting xxhash (from datasets==2.20.0->ByProt==1.0.0)                  
  Using cached xxhash-3.6.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17   
  _x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)                        
  Collecting multiprocess (from datasets==2.20.0->ByProt==1.0.0)            
  Downloading multiprocess-0.70.19-py39-none-any.whl.metadata (7.5 kB)      
  Collecting fsspec<=2024.5.0,>=2023.1.0 (from                              
  fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->ByProt==1.0.0)       
  Using cached fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)            
  Collecting aiohttp (from datasets==2.20.0->ByProt==1.0.0)                 
  Downloading aiohttp-3.13.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_1   
  7_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)                      
  Collecting huggingface-hub>=0.21.2 (from datasets==2.20.0->ByProt==1.0.0) 
  Downloading huggingface_hub-1.3.5-py3-none-any.whl.metadata (13 kB)       
  Collecting packaging (from datasets==2.20.0->ByProt==1.0.0)               
  Downloading packaging-26.0-py3-none-any.whl.metadata (3.3 kB)             
  Collecting pyyaml>=5.1 (from datasets==2.20.0->ByProt==1.0.0)             
  Using cached pyyaml-6.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17   
  _x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)                       
  Collecting hjson (from deepspeed==0.14.4->ByProt==1.0.0)                  
  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)               
  Collecting ninja (from deepspeed==0.14.4->ByProt==1.0.0)                  
  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_   
  x86_64.whl.metadata (5.1 kB)                                              
  Collecting nvidia-ml-py (from deepspeed==0.14.4->ByProt==1.0.0)           
  Downloading nvidia_ml_py-13.590.48-py3-none-any.whl.metadata (9.8 kB)     
  Collecting psutil (from deepspeed==0.14.4->ByProt==1.0.0)                 
  Downloading psutil-7.2.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_   
  x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)                         
  Collecting py-cpuinfo (from deepspeed==0.14.4->ByProt==1.0.0)             
  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)       
  Collecting pydantic (from deepspeed==0.14.4->ByProt==1.0.0)               
  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)            
  Requirement already satisfied: torch in /mnt/disk12/user/adprabh1/minicon 
  da3/envs/dplm-scaling/lib/python3.9/site-packages (from                   
  deepspeed==0.14.4->ByProt==1.0.0) (2.2.0+cu121)                           
  Collecting colorlog (from hydra-colorlog==1.2.0->ByProt==1.0.0)           
  Using cached colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)            
  Collecting omegaconf~=2.2 (from hydra-core==1.2.0->ByProt==1.0.0)         
  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)           
  Collecting antlr4-python3-runtime==4.9.* (from                            
  hydra-core==1.2.0->ByProt==1.0.0)                                         
  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl                
  Collecting optuna<3.0.0,>=2.10.0 (from                                    
  hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)                               
  Using cached optuna-2.10.1-py3-none-any.whl.metadata (15 kB)              
  Collecting lightning-utilities<2.0,>=0.8.0 (from                          
  lightning==2.2.0->ByProt==1.0.0)                                          
  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7    
  kB)                                                                       
  Collecting packaging (from datasets==2.20.0->ByProt==1.0.0)               
  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)            
  Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /mnt/disk 
  12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages 
  (from lightning==2.2.0->ByProt==1.0.0) (4.15.0)                           
  Collecting accelerate>=0.21.0 (from peft==0.11.1->ByProt==1.0.0)          
  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)          
  Collecting safetensors (from peft==0.11.1->ByProt==1.0.0)                 
  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinu   
  x2014_x86_64.whl.metadata (4.1 kB)                                        
  Collecting torchdata==0.7.1 (from torchtext==0.17.0->ByProt==1.0.0)       
  Using cached torchdata-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2   
  014_x86_64.whl.metadata (13 kB)                                           
  Requirement already satisfied: networkx in /mnt/disk12/user/adprabh1/mini 
  conda3/envs/dplm-scaling/lib/python3.9/site-packages (from                
  torch->deepspeed==0.14.4->ByProt==1.0.0) (3.2.1)                          
  Requirement already satisfied: jinja2 in /mnt/disk12/user/adprabh1/minico 
  nda3/envs/dplm-scaling/lib/python3.9/site-packages (from                  
  torch->deepspeed==0.14.4->ByProt==1.0.0) (3.1.6)                          
  Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in        
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.105)       
  Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in      
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.105)       
  Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in        
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.105)       
  Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/disk12 
  /user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages   
  (from torch->deepspeed==0.14.4->ByProt==1.0.0) (8.9.2.26)                 
  Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/disk1 
  2/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages  
  (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.3.1)                 
  Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/disk1 
  2/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages  
  (from torch->deepspeed==0.14.4->ByProt==1.0.0) (11.0.2.54)                
  Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in          
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (10.3.2.106)     
  Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in        
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (11.4.5.107)     
  Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in        
  /mnt/disk12/user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site 
  -packages (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.0.106)     
  Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /mnt/disk12/us 
  er/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages      
  (from torch->deepspeed==0.14.4->ByProt==1.0.0) (2.19.3)                   
  Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/disk12/ 
  user/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages    
  (from torch->deepspeed==0.14.4->ByProt==1.0.0) (12.1.105)                 
  Requirement already satisfied: triton==2.2.0 in /mnt/disk12/user/adprabh1 
  /miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from           
  torch->deepspeed==0.14.4->ByProt==1.0.0) (2.2.0)                          
  Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/disk12/user/ 
  adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from n 
  vidia-cusolver-cu12==11.4.5.107->torch->deepspeed==0.14.4->ByProt==1.0.0) 
  (12.9.86)                                                                 
  Requirement already satisfied: urllib3>=1.25 in /mnt/disk12/user/adprabh1 
  /miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from           
  torchdata==0.7.1->torchtext==0.17.0->ByProt==1.0.0) (1.26.13)             
  Collecting huggingface-hub>=0.21.2 (from datasets==2.20.0->ByProt==1.0.0) 
  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)     
  Collecting regex!=2019.12.17 (from transformers==4.39.2->ByProt==1.0.0)   
  Downloading regex-2026.1.15-cp39-cp39-manylinux2014_x86_64.manylinux_2_   
  17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)                      
  Collecting tokenizers<0.19,>=0.14 (from                                   
  transformers==4.39.2->ByProt==1.0.0)                                      
  Using cached tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinu   
  x2014_x86_64.whl.metadata (6.7 kB)                                        
  Collecting hf-xet<2.0.0,>=1.1.3 (from                                     
  huggingface-hub>=0.21.2->datasets==2.20.0->ByProt==1.0.0)                 
  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014   
  _x86_64.whl.metadata (4.9 kB)                                             
  Requirement already satisfied: setuptools in /mnt/disk12/user/adprabh1/mi 
  niconda3/envs/dplm-scaling/lib/python3.9/site-packages (from              
  lightning-utilities<2.0,>=0.8.0->lightning==2.2.0->ByProt==1.0.0)         
  (80.9.0)                                                                  
  Collecting alembic (from                                                  
  optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)        
  Using cached alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)            
  Collecting cliff (from                                                    
  optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)        
  Using cached cliff-4.9.1-py3-none-any.whl.metadata (2.1 kB)               
  Collecting cmaes>=0.8.2 (from                                             
  optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)        
  Using cached cmaes-0.12.0-py3-none-any.whl.metadata (29 kB)               
  Collecting scipy!=1.4.0 (from                                             
  optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)        
  Using cached scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014   
  _x86_64.whl.metadata (60 kB)                                              
  Collecting sqlalchemy>=1.1.0 (from                                        
  optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)        
  Downloading sqlalchemy-2.0.46-cp39-cp39-manylinux2014_x86_64.manylinux_   
  2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)                   
  Collecting aiohappyeyeballs>=2.5.0 (from                                  
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)    
  Collecting aiosignal>=1.4.0 (from                                         
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)           
  Collecting async-timeout<6.0,>=4.0 (from                                  
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)       
  Collecting attrs>=17.3.0 (from aiohttp->datasets==2.20.0->ByProt==1.0.0)  
  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)               
  Collecting frozenlist>=1.1.1 (from                                        
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached frozenlist-1.8.0-cp39-cp39-manylinux1_x86_64.manylinux_2_2   
  8_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)                        
  Collecting multidict<7.0,>=4.5 (from                                      
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Downloading multidict-6.7.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_   
  17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)                     
  Collecting propcache>=0.2.0 (from                                         
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached propcache-0.4.1-cp39-cp39-manylinux2014_x86_64.manylinux_2   
  _17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)                     
  Collecting yarl<2.0,>=1.17.0 (from                                        
  aiohttp->datasets==2.20.0->ByProt==1.0.0)                                 
  Using cached yarl-1.22.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_   
  x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)                         
  Requirement already satisfied: idna>=2.0 in /mnt/disk12/user/adprabh1/min 
  iconda3/envs/dplm-scaling/lib/python3.9/site-packages (from               
  yarl<2.0,>=1.17.0->aiohttp->datasets==2.20.0->ByProt==1.0.0) (3.4)        
  Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/disk12/us 
  er/adprabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages      
  (from requests>=2.32.2->datasets==2.20.0->ByProt==1.0.0) (2.1.1)          
  Requirement already satisfied: certifi>=2017.4.17 in /mnt/disk12/user/adp 
  rabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from      
  requests>=2.32.2->datasets==2.20.0->ByProt==1.0.0) (2022.12.7)            
  Collecting greenlet>=1 (from sqlalchemy>=1.1.0->optuna<3.0.0,>=2.10.0->hy 
  dra-optuna-sweeper==1.2.0->ByProt==1.0.0)                                 
  Using cached greenlet-3.2.4-cp39-cp39-manylinux_2_24_x86_64.manylinux_2   
  _28_x86_64.whl.metadata (4.1 kB)                                          
  Collecting Mako (from alembic->optuna<3.0.0,>=2.10.0->hydra-optuna-sweepe 
  r==1.2.0->ByProt==1.0.0)                                                  
  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)               
  Collecting tomli (from alembic->optuna<3.0.0,>=2.10.0->hydra-optuna-sweep 
  er==1.2.0->ByProt==1.0.0)                                                 
  Downloading tomli-2.4.0-py3-none-any.whl.metadata (10 kB)                 
  Collecting numpy (from biopython==1.79->ByProt==1.0.0)                    
  Using cached numpy-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_   
  x86_64.whl.metadata (60 kB)                                               
  Collecting msgpack>=0.5.6 (from biotite->ByProt==1.0.0)                   
  Using cached msgpack-1.1.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_1   
  7_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)                      
  Collecting click>=8.0.0 (from black->ByProt==1.0.0)                       
  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)               
  Collecting mypy-extensions>=0.4.3 (from black->ByProt==1.0.0)             
  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)     
  Collecting pathspec>=0.9.0 (from black->ByProt==1.0.0)                    
  Downloading pathspec-1.0.4-py3-none-any.whl.metadata (13 kB)              
  Collecting platformdirs>=2 (from black->ByProt==1.0.0)                    
  Using cached platformdirs-4.4.0-py3-none-any.whl.metadata (12 kB)         
  Collecting pytokens>=0.3.0 (from black->ByProt==1.0.0)                    
  Downloading pytokens-0.4.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_1   
  7_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)                      
  Collecting autopage>=0.4.0 (from                                          
  cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0) 
  Downloading autopage-0.6.0-py3-none-any.whl.metadata (8.1 kB)             
  Collecting cmd2>=1.0.0 (from                                              
  cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0) 
  Using cached cmd2-2.7.0-py3-none-any.whl.metadata (17 kB)                 
  Collecting PrettyTable>=0.7.2 (from                                       
  cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0) 
  Using cached prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)         
  Collecting stevedore>=2.0.1 (from                                         
  cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0) 
  Using cached stevedore-5.5.0-py3-none-any.whl.metadata (2.2 kB)           
  Collecting importlib-metadata>=4.4 (from                                  
  cliff->optuna<3.0.0,>=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0) 
  Using cached importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7      
  kB)                                                                       
  Collecting pyperclip>=1.8 (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10.0 
  ->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)                             
  Using cached pyperclip-1.11.0-py3-none-any.whl.metadata (2.4 kB)          
  Collecting rich-argparse>=1.7.1 (from cmd2>=1.0.0->cliff->optuna<3.0.0,>= 
  2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)                       
  Using cached rich_argparse-1.7.2-py3-none-any.whl.metadata (14 kB)        
  Collecting wcwidth>=0.2.10 (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.10. 
  0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)                            
  Downloading wcwidth-0.5.2-py3-none-any.whl.metadata (30 kB)               
  Collecting zipp>=3.20 (from importlib-metadata>=4.4->cliff->optuna<3.0.0, 
  >=2.10.0->hydra-optuna-sweeper==1.2.0->ByProt==1.0.0)                     
  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)               
  Collecting markdown-it-py>=2.2.0 (from rich->ByProt==1.0.0)               
  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)      
  Collecting pygments<3.0.0,>=2.13.0 (from rich->ByProt==1.0.0)             
  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)           
  Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->ByProt==1.0.0)   
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)               
  Collecting opt_einsum_fx>=0.1.4 (from e3nn->ByProt==1.0.0)                
  Using cached opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)       
  Collecting mccabe<0.8.0,>=0.7.0 (from flake8->ByProt==1.0.0)              
  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)          
  Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8->ByProt==1.0.0)       
  Using cached pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)    
  Collecting pyflakes<3.5.0,>=3.4.0 (from flake8->ByProt==1.0.0)            
  Using cached pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)        
  Requirement already satisfied: MarkupSafe>=2.0 in /mnt/disk12/user/adprab 
  h1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from         
  jinja2->torch->deepspeed==0.14.4->ByProt==1.0.0) (2.1.5)                  
  Collecting contourpy>=1.0.1 (from matplotlib->ByProt==1.0.0)              
  Using cached contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2   
  014_x86_64.whl.metadata (5.4 kB)                                          
  Collecting cycler>=0.10 (from matplotlib->ByProt==1.0.0)                  
  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)             
  Collecting fonttools>=4.22.0 (from matplotlib->ByProt==1.0.0)             
  Using cached fonttools-4.60.2-cp39-cp39-manylinux2014_x86_64.manylinux_   
  2_17_x86_64.whl.metadata (113 kB)                                         
  Collecting kiwisolver>=1.3.1 (from matplotlib->ByProt==1.0.0)             
  Using cached kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux   
  2010_x86_64.whl.metadata (6.3 kB)                                         
  Requirement already satisfied: pillow>=8 in /mnt/disk12/user/adprabh1/min 
  iconda3/envs/dplm-scaling/lib/python3.9/site-packages (from               
  matplotlib->ByProt==1.0.0) (11.3.0)                                       
  Collecting pyparsing>=2.3.1 (from matplotlib->ByProt==1.0.0)              
  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)            
  Collecting python-dateutil>=2.7 (from matplotlib->ByProt==1.0.0)          
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata    
  (8.4 kB)                                                                  
  Collecting importlib-resources>=3.2.0 (from matplotlib->ByProt==1.0.0)    
  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9     
  kB)                                                                       
  Collecting six>=1.5 (from                                                 
  python-dateutil>=2.7->matplotlib->ByProt==1.0.0)                          
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)            
  Collecting numpy (from biopython==1.79->ByProt==1.0.0)                    
  Using cached numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014   
  _x86_64.whl.metadata (61 kB)                                              
  Collecting GridDataFormats>=0.4.0 (from MDAnalysis->ByProt==1.0.0)        
  Using cached GridDataFormats-1.0.2-py3-none-any.whl.metadata (4.9 kB)     
  Collecting mmtf-python>=1.0.0 (from MDAnalysis->ByProt==1.0.0)            
  Using cached mmtf_python-1.1.3-py2.py3-none-any.whl.metadata (1.2 kB)     
  Collecting joblib>=0.12 (from MDAnalysis->ByProt==1.0.0)                  
  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)              
  Collecting threadpoolctl (from MDAnalysis->ByProt==1.0.0)                 
  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)        
  Collecting fasteners (from MDAnalysis->ByProt==1.0.0)                     
  Using cached fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)            
  Collecting mda-xdrlib (from MDAnalysis->ByProt==1.0.0)                    
  Using cached mda_xdrlib-0.2.0-py3-none-any.whl.metadata (19 kB)           
  Collecting mrcfile (from                                                  
  GridDataFormats>=0.4.0->MDAnalysis->ByProt==1.0.0)                        
  Using cached mrcfile-1.5.4-py2.py3-none-any.whl.metadata (7.0 kB)         
  Collecting netCDF4 (from mdtraj->ByProt==1.0.0)                           
  Using cached netCDF4-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux201   
  4_x86_64.whl.metadata (1.8 kB)                                            
  Collecting absl-py (from ml-collections->ByProt==1.0.0)                   
  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)             
  Collecting contextlib2 (from ml-collections->ByProt==1.0.0)               
  Using cached contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)    
  Collecting ihm>=2.6 (from modelcif->ByProt==1.0.0)                        
  Downloading ihm-2.9.tar.gz (399 kB)                                       
  Installing build dependencies ... done                                    
  Getting requirements to build wheel ... done                              
  Preparing metadata (pyproject.toml) ... done                              
  INFO: pip is looking at multiple versions of multiprocess to determine    
  which version is compatible with other requirements. This could take a    
  while.                                                                    
  Collecting multiprocess (from datasets==2.20.0->ByProt==1.0.0)            
  Using cached multiprocess-0.70.18-py39-none-any.whl.metadata (7.5 kB)     
  Using cached multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)     
  Using cached multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)     
  Collecting nbformat (from nbstripout->ByProt==1.0.0)                      
  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)           
  Collecting fastjsonschema>=2.15 (from                                     
  nbformat->nbstripout->ByProt==1.0.0)                                      
  Using cached fastjsonschema-2.21.2-py3-none-any.whl.metadata (2.3 kB)     
  Collecting jsonschema>=2.6 (from nbformat->nbstripout->ByProt==1.0.0)     
  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)         
  Collecting jupyter-core!=5.0.*,>=4.12 (from                               
  nbformat->nbstripout->ByProt==1.0.0)                                      
  Using cached jupyter_core-5.8.1-py3-none-any.whl.metadata (1.6 kB)        
  Collecting traitlets>=5.1 (from nbformat->nbstripout->ByProt==1.0.0)      
  Using cached traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)           
  Collecting jsonschema-specifications>=2023.03.6 (from                     
  jsonschema>=2.6->nbformat->nbstripout->ByProt==1.0.0)                     
  Using cached                                                              
  jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)     
  Collecting referencing>=0.28.4 (from                                      
  jsonschema>=2.6->nbformat->nbstripout->ByProt==1.0.0)                     
  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)        
  Collecting rpds-py>=0.7.1 (from                                           
  jsonschema>=2.6->nbformat->nbstripout->ByProt==1.0.0)                     
  Using cached rpds_py-0.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux20   
  14_x86_64.whl.metadata (4.2 kB)                                           
  Collecting cftime (from netCDF4->mdtraj->ByProt==1.0.0)                   
  Using cached cftime-1.6.4.post1-cp39-cp39-manylinux_2_17_x86_64.manylin   
  ux2014_x86_64.whl.metadata (8.7 kB)                                       
  Collecting pytz>=2020.1 (from pandas->ByProt==1.0.0)                      
  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)            
  Collecting tzdata>=2022.7 (from pandas->ByProt==1.0.0)                    
  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)         
  Collecting cfgv>=2.0.0 (from pre-commit->ByProt==1.0.0)                   
  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)            
  Collecting identify>=1.0.0 (from pre-commit->ByProt==1.0.0)               
  Using cached identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)       
  Collecting nodeenv>=0.11.1 (from pre-commit->ByProt==1.0.0)               
  Using cached nodeenv-1.10.0-py2.py3-none-any.whl.metadata (24 kB)         
  Collecting virtualenv>=20.10.0 (from pre-commit->ByProt==1.0.0)           
  Downloading virtualenv-20.36.1-py3-none-any.whl.metadata (4.7 kB)         
  Collecting distlib<1,>=0.3.7 (from                                        
  virtualenv>=20.10.0->pre-commit->ByProt==1.0.0)                           
  Using cached distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)         
  Collecting jedi<1,>=0.18 (from pudb->ByProt==1.0.0)                       
  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)            
  Collecting urwid-readline (from pudb->ByProt==1.0.0)                      
  Using cached urwid_readline-0.15.1-py3-none-any.whl                       
  Collecting urwid>=2.4 (from pudb->ByProt==1.0.0)                          
  Using cached urwid-3.0.3-py3-none-any.whl.metadata (11 kB)                
  Collecting parso<0.9.0,>=0.8.4 (from jedi<1,>=0.18->pudb->ByProt==1.0.0)  
  Using cached parso-0.8.5-py2.py3-none-any.whl.metadata (8.3 kB)           
  Collecting annotated-types>=0.6.0 (from                                   
  pydantic->deepspeed==0.14.4->ByProt==1.0.0)                               
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)      
  Collecting pydantic-core==2.41.5 (from                                    
  pydantic->deepspeed==0.14.4->ByProt==1.0.0)                               
  Using cached pydantic_core-2.41.5-cp39-cp39-manylinux_2_17_x86_64.manyl   
  inux2014_x86_64.whl.metadata (7.3 kB)                                     
  Collecting typing-inspection>=0.4.2 (from                                 
  pydantic->deepspeed==0.14.4->ByProt==1.0.0)                               
  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)   
  Collecting exceptiongroup>=1 (from pytest->ByProt==1.0.0)                 
  Using cached exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)      
  Collecting iniconfig>=1 (from pytest->ByProt==1.0.0)                      
  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)           
  Collecting pluggy<2,>=1.5 (from pytest->ByProt==1.0.0)                    
  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)              
  Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/disk12/user/adp 
  rabh1/miniconda3/envs/dplm-scaling/lib/python3.9/site-packages (from      
  sympy->ByProt==1.0.0) (1.3.0)                                             
  Collecting grpcio>=1.48.2 (from tensorboard->ByProt==1.0.0)               
  Using cached grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_1   
  7_x86_64.whl.metadata (3.7 kB)                                            
  Collecting markdown>=2.6.8 (from tensorboard->ByProt==1.0.0)              
  Using cached markdown-3.9-py3-none-any.whl.metadata (5.1 kB)              
  Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->ByProt==1.0.0)    
  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl.metadata   
  (593 bytes)                                                               
  Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from                    
  tensorboard->ByProt==1.0.0)                                               
  Using cached                                                              
  tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata 
  (1.1 kB)                                                                  
  Collecting werkzeug>=1.0.1 (from tensorboard->ByProt==1.0.0)              
  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)             
  Using cached                                                              
  biopython-1.79-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl   
  (2.7 MB)                                                                  
  Using cached datasets-2.20.0-py3-none-any.whl (547 kB)                    
  Using cached hydra_colorlog-1.2.0-py3-none-any.whl (3.6 kB)               
  Using cached hydra_core-1.2.0-py3-none-any.whl (151 kB)                   
  Using cached hydra_optuna_sweeper-1.2.0-py3-none-any.whl (8.5 kB)         
  Using cached lightning-2.2.0-py3-none-any.whl (2.1 MB)                    
  Using cached peft-0.11.1-py3-none-any.whl (251 kB)                        
  Using cached pytorch_lightning-2.2.0-py3-none-any.whl (800 kB)            
  Using cached torchtext-0.17.0-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)    
  Using cached                                                              
  torchdata-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  
  (4.7 MB)                                                                  
  Using cached transformers-4.39.2-py3-none-any.whl (8.8 MB)                
  Using cached dill-0.3.8-py3-none-any.whl (116 kB)                         
  Using cached fsspec-2024.5.0-py3-none-any.whl (316 kB)                    
  Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)             
  Using cached                                                              
  hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl     
  (3.3 MB)                                                                  
  Using cached lightning_utilities-0.15.2-py3-none-any.whl (29 kB)          
  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)                     
  Using cached optuna-2.10.1-py3-none-any.whl (308 kB)                      
  Using cached packaging-24.2-py3-none-any.whl (65 kB)                      
  Using cached pyyaml-6.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x 
  86_64.manylinux_2_28_x86_64.whl (750 kB)                                  
  Using cached tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2 
  014_x86_64.whl (3.6 MB)                                                   
  Using cached torchmetrics-1.8.2-py3-none-any.whl (983 kB)                 
  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)                         
  Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)                  
  Downloading aiohttp-3.13.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_ 
  x86_64.manylinux_2_28_x86_64.whl (1.7 MB)                                 
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 79.7 MB/s  0:00:00    
  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)                
  Downloading multidict-6.7.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17 
  _x86_64.manylinux_2_28_x86_64.whl (240 kB)                                
  Using cached yarl-1.22.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x8 
  6_64.manylinux_2_28_x86_64.whl (346 kB)                                   
  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)              
  Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)                    
  Using cached attrs-25.4.0-py3-none-any.whl (67 kB)                        
  Using cached cmaes-0.12.0-py3-none-any.whl (64 kB)                        
  Using cached frozenlist-1.8.0-cp39-cp39-manylinux1_x86_64.manylinux_2_28_ 
  x86_64.manylinux_2_5_x86_64.whl (219 kB)                                  
  Using cached propcache-0.4.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_1 
  7_x86_64.manylinux_2_28_x86_64.whl (197 kB)                               
  Using cached pyarrow-21.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (42.7 MB) 
  Downloading regex-2026.1.15-cp39-cp39-manylinux2014_x86_64.manylinux_2_17 
  _x86_64.manylinux_2_28_x86_64.whl (791 kB)                                
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 791.3/791.3 kB 71.3 MB/s         
  0:00:00                                                                   
  Using cached requests-2.32.5-py3-none-any.whl (64 kB)                     
  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2 
  014_x86_64.whl (507 kB)                                                   
  Using cached                                                              
  scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl     
  (38.6 MB)                                                                 
  Downloading sqlalchemy-2.0.46-cp39-cp39-manylinux2014_x86_64.manylinux_2_ 
  17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)                              
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 84.9 MB/s  0:00:00    
  Using cached                                                              
  greenlet-3.2.4-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl  
  (582 kB)                                                                  
  Using cached alembic-1.16.5-py3-none-any.whl (247 kB)                     
  Using cached msgpack-1.1.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_ 
  x86_64.manylinux_2_28_x86_64.whl (404 kB)                                 
  Using cached black-25.11.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_ 
  x86_64.manylinux_2_28_x86_64.whl (1.7 MB)                                 
  Using cached click-8.1.8-py3-none-any.whl (98 kB)                         
  Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)              
  Downloading pathspec-1.0.4-py3-none-any.whl (55 kB)                       
  Using cached platformdirs-4.4.0-py3-none-any.whl (18 kB)                  
  Downloading pytokens-0.4.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_ 
  x86_64.manylinux_2_28_x86_64.whl (259 kB)                                 
  Downloading tomli-2.4.0-py3-none-any.whl (14 kB)                          
  Using cached cliff-4.9.1-py3-none-any.whl (84 kB)                         
  Downloading autopage-0.6.0-py3-none-any.whl (30 kB)                       
  Using cached cmd2-2.7.0-py3-none-any.whl (154 kB)                         
  Using cached importlib_metadata-8.7.1-py3-none-any.whl (27 kB)            
  Using cached prettytable-3.16.0-py3-none-any.whl (33 kB)                  
  Using cached pyperclip-1.11.0-py3-none-any.whl (11 kB)                    
  Using cached rich_argparse-1.7.2-py3-none-any.whl (25 kB)                 
  Downloading rich-14.3.1-py3-none-any.whl (309 kB)                         
  Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)                    
  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)                
  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)                       
  Using cached stevedore-5.5.0-py3-none-any.whl (49 kB)                     
  Downloading wcwidth-0.5.2-py3-none-any.whl (90 kB)                        
  Using cached zipp-3.23.0-py3-none-any.whl (10 kB)                         
  Using cached colorlog-6.10.1-py3-none-any.whl (11 kB)                     
  Downloading debugpy-1.8.20-cp39-cp39-manylinux_2_34_x86_64.whl (3.1 MB)   
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 83.6 MB/s  0:00:00    
  Using cached                                                              
  dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl    
  (153 kB)                                                                  
  Using cached e3nn-0.5.9-py3-none-any.whl (450 kB)                         
  Using cached opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)                 
  Downloading einops-0.8.2-py3-none-any.whl (65 kB)                         
  Using cached fair_esm-2.0.0-py3-none-any.whl (93 kB)                      
  Using cached flake8-7.3.0-py2.py3-none-any.whl (57 kB)                    
  Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)                   
  Using cached pycodestyle-2.14.0-py2.py3-none-any.whl (31 kB)              
  Using cached pyflakes-3.4.0-py2.py3-none-any.whl (63 kB)                  
  Using cached hjson-3.1.0-py3-none-any.whl (54 kB)                         
  Using cached isort-6.1.0-py3-none-any.whl (94 kB)                         
  Using cached lmdb-1.7.5-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86 
  _64.manylinux_2_28_x86_64.whl (291 kB)                                    
  Using cached mako-1.3.10-py3-none-any.whl (78 kB)                         
  Using cached                                                              
  matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl 
  (8.3 MB)                                                                  
  Using cached                                                              
  contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  
  (321 kB)                                                                  
  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)                      
  Using cached                                                              
  fonttools-4.60.2-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl 
  (4.8 MB)                                                                  
  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)           
  Using cached                                                              
  kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl 
  (1.6 MB)                                                                  
  Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)                     
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)    
  Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)                      
  Using cached                                                              
  MDAnalysis-2.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl 
  (10.1 MB)                                                                 
  Using cached                                                              
  numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl     
  (18.2 MB)                                                                 
  Using cached GridDataFormats-1.0.2-py3-none-any.whl (2.1 MB)              
  Using cached joblib-1.5.3-py3-none-any.whl (309 kB)                       
  Using cached mmtf_python-1.1.3-py2.py3-none-any.whl (25 kB)               
  Using cached fasteners-0.20-py3-none-any.whl (18 kB)                      
  Using cached mda_xdrlib-0.2.0-py3-none-any.whl (14 kB)                    
  Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)                      
  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)              
  Using cached modelcif-1.6-py3-none-any.whl (47 kB)                        
  Using cached mrcfile-1.5.4-py2.py3-none-any.whl (45 kB)                   
  Using cached multiprocess-0.70.16-py39-none-any.whl (133 kB)              
  Using cached nbstripout-0.8.2-py2.py3-none-any.whl (17 kB)                
  Using cached nbformat-5.10.4-py3-none-any.whl (78 kB)                     
  Using cached fastjsonschema-2.21.2-py3-none-any.whl (24 kB)               
  Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)                   
  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)  
  Using cached jupyter_core-5.8.1-py3-none-any.whl (28 kB)                  
  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)                  
  Using cached                                                              
  rpds_py-0.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl   
  (384 kB)                                                                  
  Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)                    
  Using cached                                                              
  netCDF4-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl    
  (9.1 MB)                                                                  
  Using cached cftime-1.6.4.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux 
  2014_x86_64.whl (1.3 MB)                                                  
  Using cached                                                              
  ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 
  kB)                                                                       
  Downloading nvidia_ml_py-13.590.48-py3-none-any.whl (50 kB)               
  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)                    
  Using cached                                                              
  pandas-2.3.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl    
  (12.8 MB)                                                                 
  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)                    
  Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)                  
  Using cached pre_commit-4.3.0-py2.py3-none-any.whl (220 kB)               
  Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)                     
  Using cached identify-2.6.15-py2.py3-none-any.whl (99 kB)                 
  Using cached nodeenv-1.10.0-py2.py3-none-any.whl (23 kB)                  
  Downloading virtualenv-20.36.1-py3-none-any.whl (6.0 MB)                  
  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 83.5 MB/s  0:00:00    
  Using cached distlib-0.4.0-py2.py3-none-any.whl (469 kB)                  
  Downloading psutil-7.2.2-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x8 
  6_64.manylinux_2_28_x86_64.whl (155 kB)                                   
  Using cached pudb-2025.1.5-py3-none-any.whl (92 kB)                       
  Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)                    
  Using cached parso-0.8.5-py2.py3-none-any.whl (106 kB)                    
  Using cached urwid-3.0.3-py3-none-any.whl (296 kB)                        
  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)                    
  Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)                 
  Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)                    
  Using cached pydantic_core-2.41.5-cp39-cp39-manylinux_2_17_x86_64.manylin 
  ux2014_x86_64.whl (2.1 MB)                                                
  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)               
  Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)             
  Using cached pyrootutils-1.0.4-py3-none-any.whl (5.8 kB)                  
  Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)                 
  Using cached pytest-8.4.2-py3-none-any.whl (365 kB)                       
  Using cached pluggy-1.6.0-py3-none-any.whl (20 kB)                        
  Using cached exceptiongroup-1.3.1-py3-none-any.whl (16 kB)                
  Using cached iniconfig-2.1.0-py3-none-any.whl (6.0 kB)                    
  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux 
  2014_x86_64.whl (13.5 MB)                                                 
  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)                 
  Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)                     
  Using cached sh-2.2.2-py3-none-any.whl (38 kB)                            
  Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)                 
  Using cached                                                              
  tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB) 
  Using cached                                                              
  grpcio-1.76.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl    
  (6.6 MB)                                                                  
  Using cached markdown-3.9-py3-none-any.whl (107 kB)                       
  Downloading protobuf-6.33.5-cp39-abi3-manylinux2014_x86_64.whl (323 kB)   
  Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)                      
  Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)              
  Using cached xxhash-3.6.0-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x 
  86_64.manylinux_2_28_x86_64.whl (192 kB)                                  
  Building wheels for collected packages: ByProt, ihm                       
  Building editable for ByProt (pyproject.toml) ... done                    
  Created wheel for ByProt:                                                 
  filename=byprot-1.0.0-0.editable-py3-none-any.whl size=5803               
  sha256=f3546dc4ad8895948655b82519abe85ae3745fa46e790b8eb3df198585410ce2   
  Stored in directory: /tmp/pip-ephem-wheel-cache-tvcvwssc/wheels/bc/43/b   
  d/98d9d4cb82df118f95891856a1ab05619e3581dddf3cb43b3d                      
  Building wheel for ihm (pyproject.toml) ... done                          
  Created wheel for ihm: filename=ihm-2.9-cp39-cp39-linux_x86_64.whl        
  size=238176                                                               
  sha256=6fc835b1365782a9f794f6f92e41a9a1048719d501c93e1684ca17b6aa91d5d2   
  Stored in directory: /home/adprabh1/.cache/pip/wheels/ac/df/4b/97a4ff0c   
  1b5c2b6d9ebcdde33fb326e1bfec6671e9dea576cd                                
  Successfully built ByProt ihm                                             
  Installing collected packages: pytz, pyperclip, py-cpuinfo, nvidia-ml-py, 
  lmdb, hjson, fastjsonschema, fair-esm, dm-tree, distlib,                  
  antlr4-python3-runtime, zipp, xxhash, werkzeug, wcwidth, tzdata,          
  typing-inspection, traitlets, tqdm, tomli, threadpoolctl,                 
  tensorboard-data-server, stevedore, six, sh, safetensors, rpds-py,        
  requests, regex, pyyaml, pytokens, python-dotenv, pyparsing, pygments,    
  pyflakes, pydantic-core, pycodestyle, pyarrow-hotfix, pyarrow, psutil,    
  protobuf, propcache, pluggy, platformdirs, pathspec, parso, packaging,    
  opt_einsum, numpy, nodeenv, ninja, mypy-extensions, multidict, msgpack,   
  mdurl, mda-xdrlib, mccabe, Mako, kiwisolver, joblib, iniconfig, identify, 
  hf-xet, grpcio, greenlet, fsspec, frozenlist, fonttools, fasteners,       
  exceptiongroup, einops, dill, debugpy, cycler, contextlib2, colorlog,     
  click, cfgv, autopage, attrs, async-timeout, annotated-types,             
  aiohappyeyeballs, absl-py, yarl, virtualenv, urwid, tmtools, sqlalchemy,  
  scipy, referencing, python-dateutil, pytest, pyrootutils, pydantic,       
  PrettyTable, omegaconf, multiprocess, mrcfile, mmtf-python,               
  ml-collections, markdown-it-py, lightning-utilities, jupyter-core, jedi,  
  importlib-resources, importlib-metadata, ihm, huggingface-hub, flake8,    
  contourpy, cmaes, cftime, black, biotite, biopython, aiosignal,           
  urwid-readline, tokenizers, scikit-learn, rich, pre-commit, pandas,       
  netCDF4, modelcif, matplotlib, markdown, jsonschema-specifications,       
  isort, hydra-core, GridDataFormats, alembic, aiohttp, transformers,       
  torchmetrics, torchdata, torch_geometric, tensorboard, seaborn,           
  rich-argparse, pudb, opt_einsum_fx, mdtraj, MDAnalysis, jsonschema,       
  hydra-colorlog, deepspeed, accelerate, torchtext, pytorch_lightning,      
  peft, nbformat, e3nn, datasets, cmd2, nbstripout, lightning, cliff,       
  optuna, hydra-optuna-sweeper, ByProt                                      
  Attempting uninstall: requests                                            
  Found existing installation: requests 2.28.1                              
  Uninstalling requests-2.28.1:                                             
  Successfully uninstalled requests-2.28.1                                  
  Attempting uninstall: numpy                                               
  Found existing installation: numpy 2.0.2                                  
  Uninstalling numpy-2.0.2:                                                 
  Successfully uninstalled numpy-2.0.2                                      
  Attempting uninstall: fsspec                                              
  Found existing installation: fsspec 2025.10.0                             
  Uninstalling fsspec-2025.10.0:                                            
  Successfully uninstalled fsspec-2025.10.0                                 
  Successfully installed ByProt-1.0.0 GridDataFormats-1.0.2                 
  MDAnalysis-2.7.0 Mako-1.3.10 PrettyTable-3.16.0 absl-py-2.3.1             
  accelerate-1.10.1 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0   
  alembic-1.16.5 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3         
  async-timeout-5.0.1 attrs-25.4.0 autopage-0.6.0 biopython-1.79            
  biotite-0.40.0 black-25.11.0 cfgv-3.4.0 cftime-1.6.4.post1 click-8.1.8    
  cliff-4.9.1 cmaes-0.12.0 cmd2-2.7.0 colorlog-6.10.1 contextlib2-21.6.0    
  contourpy-1.3.0 cycler-0.12.1 datasets-2.20.0 debugpy-1.8.20              
  deepspeed-0.14.4 dill-0.3.8 distlib-0.4.0 dm-tree-0.1.8 e3nn-0.5.9        
  einops-0.8.2 exceptiongroup-1.3.1 fair-esm-2.0.0 fasteners-0.20           
  fastjsonschema-2.21.2 flake8-7.3.0 fonttools-4.60.2 frozenlist-1.8.0      
  fsspec-2024.5.0 greenlet-3.2.4 grpcio-1.76.0 hf-xet-1.2.0 hjson-3.1.0     
  huggingface-hub-0.36.0 hydra-colorlog-1.2.0 hydra-core-1.2.0              
  hydra-optuna-sweeper-1.2.0 identify-2.6.15 ihm-2.9                        
  importlib-metadata-8.7.1 importlib-resources-6.5.2 iniconfig-2.1.0        
  isort-6.1.0 jedi-0.19.2 joblib-1.5.3 jsonschema-4.25.1                    
  jsonschema-specifications-2025.9.1 jupyter-core-5.8.1 kiwisolver-1.4.7    
  lightning-2.2.0 lightning-utilities-0.15.2 lmdb-1.7.5 markdown-3.9        
  markdown-it-py-3.0.0 matplotlib-3.9.4 mccabe-0.7.0 mda-xdrlib-0.2.0       
  mdtraj-1.10.3 mdurl-0.1.2 ml-collections-0.1.1 mmtf-python-1.1.3          
  modelcif-1.6 mrcfile-1.5.4 msgpack-1.1.2 multidict-6.7.1                  
  multiprocess-0.70.16 mypy-extensions-1.1.0 nbformat-5.10.4                
  nbstripout-0.8.2 netCDF4-1.7.2 ninja-1.13.0 nodeenv-1.10.0 numpy-1.26.4   
  nvidia-ml-py-13.590.48 omegaconf-2.3.0 opt_einsum-3.4.0                   
  opt_einsum_fx-0.1.4 optuna-2.10.1 packaging-24.2 pandas-2.3.3 parso-0.8.5 
  pathspec-1.0.4 peft-0.11.1 platformdirs-4.4.0 pluggy-1.6.0                
  pre-commit-4.3.0 propcache-0.4.1 protobuf-6.33.5 psutil-7.2.2             
  pudb-2025.1.5 py-cpuinfo-9.0.0 pyarrow-21.0.0 pyarrow-hotfix-0.7          
  pycodestyle-2.14.0 pydantic-2.12.5 pydantic-core-2.41.5 pyflakes-3.4.0    
  pygments-2.19.2 pyparsing-3.3.2 pyperclip-1.11.0 pyrootutils-1.0.4        
  pytest-8.4.2 python-dateutil-2.9.0.post0 python-dotenv-1.2.1              
  pytokens-0.4.1 pytorch_lightning-2.2.0 pytz-2025.2 pyyaml-6.0.3           
  referencing-0.36.2 regex-2026.1.15 requests-2.32.5 rich-14.3.1            
  rich-argparse-1.7.2 rpds-py-0.27.1 safetensors-0.7.0 scikit-learn-1.6.1   
  scipy-1.13.1 seaborn-0.13.2 sh-2.2.2 six-1.17.0 sqlalchemy-2.0.46         
  stevedore-5.5.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2          
  threadpoolctl-3.6.0 tmtools-0.3.0 tokenizers-0.15.2 tomli-2.4.0           
  torch_geometric-2.6.1 torchdata-0.7.1 torchmetrics-1.8.2 torchtext-0.17.0 
  tqdm-4.67.1 traitlets-5.14.3 transformers-4.39.2 typing-inspection-0.4.2  
  tzdata-2025.3 urwid-3.0.3 urwid-readline-0.15.1 virtualenv-20.36.1        
  wcwidth-0.5.2 werkzeug-3.1.5 xxhash-3.6.0 yarl-1.22.0 zipp-3.23.0         
  Obtaining file:///mnt/disk12/user/adprabh1/dplm-scaling/vendor/openfold   
  Installing build dependencies ... done                                    
  Checking if build backend supports build_editable ... done                
  Getting requirements to build editable ... error                          
  error: subprocess-exited-with-error                                       
                                                                            
  × Getting requirements to build editable did not run successfully.        
  │ exit code: 1                                                            
  ╰─> [19 lines of output]                                                  
  Traceback (most recent call last):                                        
  File                                                                      
  "/home/adprabh1/data/miniconda3/envs/dplm-scaling/lib/python3.9/site-pack 
  ages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 389,   
  in <module>                                                               
  main()                                                                    
  File                                                                      
  "/home/adprabh1/data/miniconda3/envs/dplm-scaling/lib/python3.9/site-pack 
  ages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 373,   
  in main                                                                   
  json_out["return_val"] = hook(**hook_input["kwargs"])                     
  File                                                                      
  "/home/adprabh1/data/miniconda3/envs/dplm-scaling/lib/python3.9/site-pack 
  ages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py", line 157,   
  in get_requires_for_build_editable                                        
  return hook(config_settings)                                              
  File "/tmp/pip-build-env-ogc4wr60/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 479, in                              
  get_requires_for_build_editable                                           
  return self.get_requires_for_build_wheel(config_settings)                 
  File "/tmp/pip-build-env-ogc4wr60/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 333, in get_requires_for_build_wheel 
  return self._get_build_requires(config_settings,                          
  requirements=[])                                                          
  File "/tmp/pip-build-env-ogc4wr60/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 301, in _get_build_requires          
  self.run_setup()                                                          
  File "/tmp/pip-build-env-ogc4wr60/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 518, in run_setup                    
  super().run_setup(setup_script=setup_script)                              
  File "/tmp/pip-build-env-ogc4wr60/overlay/lib/python3.9/site-pack         
  ages/setuptools/build_meta.py", line 317, in run_setup                    
  exec(code, locals())                                                      
  File "<string>", line 19, in <module>                                     
  ModuleNotFoundError: No module named 'torch'                              
  [end of output]                                                           
                                                                            
  note: This error originates from a subprocess, and is likely not a        
  problem with pip.                                                         
  ERROR: Failed to build                                                    
  'file:///mnt/disk12/user/adprabh1/dplm-scaling/vendor/openfold' when      
  getting requirements to build editable                                    
  (dplm-scaling) adprabh1@wolf:~/data/dplm-scaling$                         
                                                                            

⏺ Everything installed except OpenFold (expected). Now install OpenFold with the
   fix:

  git pull   # Get the OPENFOLD_CPU_ONLY fix you pushed earlier
  OPENFOLD_CPU_ONLY=1 pip install -e vendor/openfold --no-build-isolation

  Then verify:
  python -c 'from openfold.np import residue_constants; print("OpenFold works")'

❯ ok, OpenFold works 

⏺ You now have a clean setup with only 1 code change (the OPENFOLD_CPU_ONLY
  fix).

  Summary:
  - ~/data/dplm - old repo (backup)
  - ~/data/dplm-scaling - fresh repo, minimal modifications
  - dplm-scaling conda env - clean environment

  Ready to run experiments. You can test with:

  cd ~/data/dplm-scaling

  # Quick smoke test
  python -c "from byprot.models.dplm import DiffusionProteinLanguageModel as 
  DPLM; print('DPLM imports work')"

  # Or run training (you'll need data first)
  # bash scripts/download_uniref50_hf.sh

  What would you like to do next?