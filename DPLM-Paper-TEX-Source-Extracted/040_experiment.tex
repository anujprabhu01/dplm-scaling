\section{Experiments}
\label{sec:experiment}





We evaluate \method on extensive generative and understanding tasks, spanning unconditional generation (\Secref{sec:uncond}), a variety of protein predictive downstream tasks (\Secref{sec:understanding}), and conditional tasks, including motif-scaffolding (\Secref{sec:motif}), inverse-folding task (\Secref{sec:IF}), and secondary structure guided controllable generation (\Secref{sec:controllable}). 
We find that, in general, \method with larger model scales can attain better results than smaller ones, demonstrating the scaling law can also hold for protein language modeling.
Please refer to the Appendix for more detailed experimental settings.
% \dongyu{maybe a series of conditional generation tasks including blablabla}
% 
% - nihao, ni zai ganma


\input{tables/understanding}


\subsection{Evaluation of Unconditional Generation}
\label{sec:uncond}

% We investigate whether \method can sample high quality protein sequences unconditionally. 
\Figref{fig:uncond_sample} shows the results of \method for unconditional generation, where we evaluate the performance regarding a set of lengths $[100, 200, ..., 900, 1000]$ in intervals of 100.
The reverse process of \method for sampling iterates for 500 steps.
% For the sake of fairness, we choose models of comparable sizes (650M \method and 640M EvoDiff). 
% Considering that EvoDiff has two 640m variants, we pick the OADM model which has better performance. 
Meanwhile, we also randomly pick the natural sequences of the same length from UniRef50 as reference (denoted as UR50)
We highlight our primary findings as follows:

% \paragraph{(1) Discrete diffusion is best-suited probabilistic framework for protein sequence generation, compared to \MLM and \ARLM.}

% We examine the structural plausibility or foldability of protein sequences using state-of-the-art single-sequence structure prediction model, \ie, ESMFold~\citep{lin2022esmfold}, and measured by the predicted local distance difference test (\metric{pLDDT}) score, which is considered high confidence if $\metric{pLDDT} > 70$. 

\paragraph{(1) On Foldability:} 
\ul{DPLM is capable of generating protein sequences with reasonable predicted structures.}
We examine the structural plausibility or foldability of protein sequences using the state-of-the-art single-sequence structure prediction model, \ie, ESMFold~\citep{lin2022esmfold}, and measured by the predicted local distance difference test (\metric{pLDDT}) score, which is considered high confidence if $\metric{pLDDT} > 70$. 
% Moreover, for scPPL, we use inverse folding algorithms such as ESM-IF to get the sequence probability distribution of the predicted structure, and then calculate the perplexity in relation to the original generated sequence.
We can find that protein sequences generated by \method achieve the highest \metric{pLDDT} score across all lengths (\Figref{fig:uncond_sample}A). 
Plus, secondary structure analysis of the sequences generated by \method reveals a higher proportion of beta-strands (\Figref{fig:uncond_sample}D), and overall similar to the statistics of known protein structures in Protein Data Bank~\citep[PDB;][]{berman2000protein}.
Moreover, we can see that scaling \method leads to better foldability performance, especially for very long proteins (\Figref{fig:uncond_sample}E).

% even better than sequence in UniRef50, which contains many structurally disored proteins.













\paragraph{(2) On Novelty.}
We investigate whether \method can sample sequences possessing novel structures, where we compare the structural similarity against known structures in PDB with \metric{TMScore}.
The highest TMscore is used to measure the novelty of each sequence, which we refer to as \metric{pdb-TM} score.
% We also sample sequences from EvoDiff and UniRef dataset as comparison. 
Overall, \method has relatively higher \metric{pdbTM} than EvoDiff and natural sequences, as shown in \Figref{fig:uncond_sample}B. 
Interestingly, the \metric{pdbTM} score of \method will decrease as protein gets longer than 300 while maintaining the $\metric{pLDDT} > 75$. 
% Furthermore, we can observe that for every sequence length, there is a varying proportion (25\%-80\%) of sequences where their \metric{pdbTM} is less than 0.5.
This indicates that \method possesses the ability to sample sequences with structures not similar to PDB across various lengths, with the discrepancy becoming increasingly apparent as the sequence length extends.

\paragraph{(3) On Diversity.}
We quantify the diversity of sequences sampled by \method by \metric{inner-TM} score. 
Specifically, for each sampled candidate, we use ESMFold to predict its structure and compute \metric{TMscore} against the rest.
The average \metric{TMscore} is considered as the diversity.
As shown in \Figref{fig:uncond_sample}C, \method has a considerably low average \metric{inner-TM}, demonstrating that the \method can synthesize structurally diverse sequences.

\paragraph{(4) On Learning:}
\ul{Discrete diffusion is the best-suited probabilistic framework for protein sequence generation, compared to Masked-LM and \textsc{Ar-LM}.}
As shown in \Figref{fig:uncond_sample}F, \method outperforms \MLM and \ARLM in terms of foldability, verifying our motivation to pursue a diffusion protein LM that diffusion is a more proper probabilistic framework for protein modeling.
Moreover, \ARLM also falls short of precisely controlling the length of sampled sequences, making it less flexible in practice.
As revealed in \Figref{fig:uncond_sample}G, we find that despite attaining improved generation quality over ESM2 with directly pre-training \method from scratch (\method-FS), it can bring additional learning challenges and training overheads. 
As such, we leverage a 2-stage training strategy, which consists of masked language modeling as the first stage objective, followed by diffusion objective, solving this problem and obtaining high-quality generation with \metric{pLDDT} closely approaching 90.

\paragraph{(5) Case Study.}
In \Figref{fig:uncond_sample}H, we showcase proteins sampled by \method across various lengths, ranging from 100 to 1000, while more cases are presented in the Appendix.
As the protein gets longer, the complexity of its structure will increase, containing rich helices and sheets.
We also find that \method can sample proteins composed of tandem repeats such as beta-barrel or Kelch repeat domain.

% In short, \method exhibits the proficiency to generate protein sequences of high structural plausibility, along with diversity and novelty. 
% As sequences get longer, the novelty will further increase while still maintaining a high foldability.




% \begin{figure}[t]
%     \centering
%     % %\vspace{-0.5mm}
%     \!\!\!\includegraphics[width=0.9\linewidth]{figures/plddt.png}
%     %\vspace{-10pt}
%     \caption{The pLDDT scores of \method, EvoDiff and natural sequences with different protein lengths. \method achieves the highest pLDDT score across all lengths.}
%     \label{fig:plddt}
%     %\vspace{-2mm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     % %\vspace{-0.5mm}
%     \!\!\!\includegraphics[width=0.9\linewidth]{figures/scppl.png}
%     %\vspace{-10pt}
%     \caption{The self-consistency perplexity scores of \method, EvoDiff and natural sequences with different protein lengths. \method achieves the lowest scPPL score across all lengths.}
%     \label{fig:scppl}
%     %\vspace{-2mm}
% \end{figure}






% \input{tables/motif_success}

\begin{figure*}[t!]
    \centering
    \vspace{-1mm}
    \includegraphics[width=0.96\linewidth]{figures/dplm_motif_all_3.pdf}
    \vspace{-4mm}
    \caption{{\sl Evaluation of motif-scaffolding.}
    \textbf{(A)} comparison regarding overall success rate and number of solved problems;
    \textbf{(B)} comparison between sequence-only approaches (\method \vs EvoDiff);  
    \textbf{(C)} comparison between sequence-only \vs structure-conditioned \method; and
    \textbf{(D)} comparison between \method (structure-conditioned and sequence-only \method) \vs RFDffusion;
    \textbf{(E)} case study for three problems.
    }
    \label{fig:evodiff_difflm_motif}
    \vspace{-3mm}
\end{figure*}








\subsection{Evaluation of Protein Representation Learning on Downstream Predictive Tasks}
\label{sec:understanding}
% \paragraph{Experimental Settings.}

We evaluate \method across a variety of protein predictive tasks~ \citep{su2023saprot,dallago2021flip,xu2022peer}, including protein function prediction (Thermostability and Metal Ion Binding), protein localization prediction (DeepLoc), protein annotation prediction (EC and GO), protein-protein interaction prediction (HumanPPI), where we perform full-parameters supervised fine-tuning on each dataset.
We also include linear probing for secondary structure from TAPE~\citep{rao2019evaluating}.
% We follow \citet{su2023saprot} to collect and split the dataset. 
% \zzx{TODO: add training details.}

% For training hyperparameters, we use the AdamW optimizer with $\beta_1=0.9$ and $\beta_2=0.98$, while weight dacay is 0.01.
% We utilize the pretrained parameters of \method and fine-tune all the parameters during training.
% We set the original ESM2 as our baseline, and compare \method with ESM2 on the each task.

\paragraph{\method is a superior protein sequence representation learner.}
As demonstrated in \Tabref{tab:results_understanding}, \method outperforms ESM2 across all tasks.
This improved performance is due to the proposed diffusion pre-training, which requires \method to adeptly learn to reconstruct the native sequence from a varied proportion of masking, including very high noise level, in contrast to ESM2 of a fixed 15\% masking ratio.
Under this circumstance, it becomes a much more challenging missing amino acid reconstruction task encouraging the model to capture the deep dependencies from the very context.
% Consequently, \method's enhanced proficiency in context information extraction contributes to its superior results in downstream tasks.
Besides, we surprisingly find that \method also closely approaches the performance of SaProt~\citep{su2023saprot}, which is a structure-aware LM that incorporates explicitly protein structures based on Foldseek~\citep{van2023foldseek} and folding models like AlphaFold~\citep{jumper2021AF2}.
This implies that \method may implicitly learn the protein structures from massive sequence data. 
Integrating explicit structural information into \method like \citet{su2023saprot} may bring further benefits, which deserve further exploration.
Our results substantiate our initial premise that \method gains a deeper understanding of protein through the generative learning process, \ie, it learns to better understand proteins by learning to generate them, leading to improved predictive performance.


% \zzx{TODO: elaborate on results, findings, and implication. }









\subsection{Evaluation of Conditional Generation}
\label{sec:exp_cond}

\subsubsection{Sequence-cond.: Motif-scaffolding}
\label{sec:motif}
% In this section, we investigate the performance of \method in supporting the structure of protein motifs that perform binding and catalytic roles. 

% Recent work has shown that the motif-scaffolding task can be completed based on the motif sequences~\citep{alamdari2023protein}. 
% We examine  \method can generate reasonable scaffold sequences given the functional motif sequence.
The goal of motif-scaffolding requires a valid scaffold to maintain the structure of the given motif such that the original function can be preserved.
Here, we follow the experimental setting in~\citet{alamdari2023protein}, where we (1) initially determine the length of a scaffold and fill the scaffold positions with the mask token; then (2) keep the motif fragment fixed during inference, and sample scaffold conditioned on the motif; and finally use OmegaFold~\citep{wu2022high} to predict the structure of the sampled sequences.
A scaffold is considered successful when it meets two conditions: (1) the \metric{RMSD} between the predicted motif structure and the ground truth, referred to as $\metric{motif-RMSD} < 1\AA$; and (2) the structure should have an overall $\metric{pLDDT} > 70$. 
Overall, we examine 17 motif-scaffolding problems, and for each problem, we sample 100 sequences and then calculate the success rate according to the above criterion.



\paragraph{\method can generate reasonable scaffolds for the given functional motifs.}
As shown in \Figref{fig:evodiff_difflm_motif}, we find that \method outperforms EvoDiff in terms of the number of solved problems and the average success rate. 
Moreover, on the problems that both \method and EvoDiff can solve, the success rate of \method is higher than EvoDiff, except \texttt{3ixt}. 
This indicates that \method excels in motif-scaffolding, preserving the motif structure during scaffold generation.
To gain more insights, we compare \method with structure conditioning (see \Secref{sec:IF}) with state-of-the-art structure designer RFDiffusion~\citep{watson2023RFdiffusion}. We find that \method shows better results in 6 problems, especially for \texttt{1PRW} and \texttt{5YUI}).
We find that utilizing motif structure helps \method make a further improvement on 4 problems compared to the original sequence-only \method, while decreasing performance on the other 6 problems. This implies that for some specific motifs, scaffolding in sequence space may be better. 
% We further analyze this phenomenon in the next section.

The detailed analysis unveiled a common biological property among the motifs observed in these two cases.
Specifically, the motif sequence displayed a remarkable level of evolutionary conservation, playing pivotal roles in binding critical signal passengers (\metric{1PRW}: calmodulin EF hand for calcium binding and \metric{5YUI}: carbonic anhydrase II for CO2 binding). 
Notably, the motif structures predominantly comprised flexible loops. 
Conversely, \metric{5TPN}, \metric{6VW1}, and \metric{2KL8}, which exhibited a distinct advantage in motif scaffolding as indicated by the RFdiffusion, featured rigid helical structures that lacked functional evolutionary conservations. This intriguing phenomenon suggests that DPLM holds great promise as a superior method for constructing structurally flexible yet evolutionarily conserved functional motif scaffolding.


% \subsection{Conditional Inpainting for Antibody CDR Design}
% \paragraph{Experimental Settings.}

% \paragraph{Results.}

% \begin{figure}[t]
%     \centering
%     % %\vspace{-0.5mm}
%     \!\!\!\includegraphics[width=0.9\linewidth]{figures/evodiff_difflm_motif.png}
%     %\vspace{-15pt}
%     \caption{Scaffold generation success rate for \method and EvoDiff.}
%     \label{fig:evodiff_difflm_motif}
%     %\vspace{-2mm}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     % %\vspace{-0.5mm}
%     \!\!\!\includegraphics[width=0.9\linewidth]{figures/rfdiffusion_difflm_motif.png}
%     %\vspace{-15pt}
%     \caption{Scaffold generation success rate for \method and RFDiffusion.}
%     \label{fig:rfdiffusion_difflm_motif}
%     %\vspace{-2mm}
% \end{figure}
% \begin{figure}[t]
%     \centering
%     % %\vspace{-0.5mm}
%     \!\!\!\includegraphics[width=0.9\linewidth]{figures/seqonly_struct_motif.png}
%     %\vspace{-15pt}
%     \caption{Scaffold generation success rate for seq-only design and structure-conditioned design.}
%     \label{fig:seq_struct_motif}
%     %\vspace{-2mm}
% \end{figure}




\input{tables/cath}

\subsubsection{Structure-conditioned: Inverse Folding}
\label{sec:IF}
% \paragraph{Experimental Settings.}
The goal of inverse folding is to find an amino acid sequence that can fold to a given protein backbone structure. 
We follow \textsc{LM-Design}~\citep{zheng2023structure} to implant a structural adapter into the last network layer of \method, and use GVP-Transformer Encoder~\citep{hsu2022esmif} as the expert protein backbone structure encoder.
% Consequently, we can utilize arbitrary pretrained structure encoder to process the 3D coordinates and provide structure information for \method.
% During training, we freeze the parameters of the structure encoder and \method, only optimize the structural adapter with the simplified discrete diffusion objective~\citep{zheng2023reparameterized}. 
% At inference time, we follow the \method generative process, except that we obtain protein sequence via greedy deterministic decoding, instead of random sampling from the distribution.
% Besides, considering that we have had an unconditional model, i.e. the \method itself, and a conditional model, i.e. the \method with structural adapter, so we can also seamlessly utilize the classifier-free guidance paradigm during inference.
We assess \method on CATH 4.2 and 4.3~\citep{orengo1997cath}.
We use amino acid recovery (\metric{AAR}) for sequence evaluation, whilst for structure evaluation, we first predict the structure of the generated sequence using ESMFold, then calculate the \metric{pLDDT} score and self-consistency TM-score (\metric{scTM}) between predicted structure and the input one.
% We compare \method with strong baselines, including the state-of-the-art approach 
% Please refer to Appendix for more detailed experimental settings and additional evaluation results.

\paragraph{\method yields sequences that can confidently fold into the given backbone structure.}
As shown in \Tabref{tab:results_cath}, \method can outperform or be on par with our strong baselines, including the state-of-the-art approach~\textsc{LM-Design}~\citep{zheng2023structure}, manifesting in \metric{AAR}, and most importantly, decent performance regarding structure evaluation ($\metric{scTM} =0.85$ and $\metric{pLDDT} > 76$).
We suggest this derives from the well-learned protein sequence knowledge of \method.
When given structure backbone information, \method can leverage this advantage and generate the sequence whose structure is both plausible and similar to the reference.







\begin{figure}[t!]
    \centering
    % \vspace{2mm}
    \includegraphics[width=\linewidth]{figures/ssp-guided.pdf}
    \vspace{-8mm}
    \caption{{\sl Secondary structure guided conditional sampling.}
    The first case contains 6 alpha-helices, The second case is much more complicated as a globally twisted structure with interleaved alpha-helices and beta-strands, where the N-terminus and C-terminus are structurally contiguous.}
    \label{fig:guided_sample}
    % \vspace{2mm}
\end{figure}

\subsubsection{Controllable Generation: Secondary Structure Guided Protein Sampling}
\label{sec:controllable}
Classifier guidance is preferred for its flexible control over the generation process without retraining for each new condition, especially beneficial in scenarios with too limited labeled data to directly attain conditional models.
Here we showcase how to guide \method to generate proteins satisfying desired secondary structures.
We train a secondary structure prediction (SSP) model as a sequence labeling task on TAPE dataset. 
We then integrate this SSP discriminative model into \method to provide guiding signals.
% For this, classifier guidance in diffusion models is especially beneficial where integration with pre-trained classifiers can steer generation towards user preference.


\paragraph{\method enjoys plug-and-play programmability.}
\Figref{fig:guided_sample} showcases that the proposed discrete classifier guidance helps steer a pre-trained \method to generate samples satisfying provided secondary structure annotations extracted from template natural proteins.
These findings suggest that \method is highly programmable, and its full potential of generative capabilities can be realized in a plug-and-play fashion, indicating that \method preserves the appealing characteristic of controllable generation inherent in diffusion models, but for discrete data.
This flexibility to swiftly adapt to the evolving needs of users across a broad spectrum of preferences is also significant in practical applications with time and computational paramount.




% \paragraph{Experimental Settings.}

% \paragraph{Results.}


