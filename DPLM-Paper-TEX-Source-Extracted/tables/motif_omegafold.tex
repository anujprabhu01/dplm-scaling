\begin{table}[t!]
   \centering
   \small
   \setlength{\tabcolsep}{2pt}
   \vspace{-3mm}
   \caption{{\sl Performance comparison between \method and different baseline approaches on CATH 4.2 and CATH 4.3 datasets.}
   {\method's results are obtained by \texttt{argmax} decoding (\ie, no sampling).}
   $\dagger$: benchmarked results are quoted from \citet{gao2022pifold}.
   }
   % \vspace{1.5pt}
   \label{tab:results_cath}

   \resizebox{\linewidth}{!}{%
   \begin{tabular}{llrccc}
   \toprule
   & \multirow{2}{*}{ Models} 
   & \multirow{2}{*}{ Trainable} 
   & \multirow{2}{*}{\metric{AAR}} 
   & \multicolumn{2}{c}{\bf struct. eval.} \\
    \cmidrule[0.3pt](lr){5-6} & & \bf Params. & & \metric{scTM}  & \metric{pLDDT} \\
   \midrule
   % StructGNN &  8.29   &   8.74   &  6.40 &   29.44 &   28.26   &  35.91   \\

   %%%%%%%%%%%%%%%%%%%%%%%%%%
   \multirowcell{7}{\rotatebox[origin=c]{90}{CATH 4.2}}
   & $^\dagger$StructTrans~\smallcitep{ingraham2019generative}                     & 1.6M/1.6M  & 35.82&   -   & - \\
   % & GCA&  7.09&  7.49  &  6.05  & 32.62 & 31.10 & 37.64 \\
   & $^\dagger$GVP~\smallcitep{jing2020gvp}                                       & 1.0M/1.0M  &  39.47 &   - & -  \\
   % & AlphaDesign &  7.32  & 7.63 &  6.30 &  34.16  & 32.66  &  41.31   \\
   & $^\dagger$ProteinMPNN~\smallcitep{dauparas2022proteinmpnn}                   & 1.9M/1.9M  &  45.96 &  -  & -    \\
   & PiFold~\smallcitep{gao2022pifold} \                                        & 6.6M/6.6M  &  {51.66} &   {-} &{-}  \\ 
  %%%%%%%%%%%%%%% OURS %%%%%%%%%%%%%%%%%%%
  \cmidrule[0.5pt](lr){2-6}
  & ProteinMPNN + CMLM                                                 & 1.9M/1.9M  &48.62     & - & - \\ 
  % & \chl \method (ProtMPNN-CMLM)                                          & \chl 6.9M/659M (\textbf{1.0\%})  &\chl 7.02 & \chl 6.67 & \chl 4.63   & \chl 35.71  & \chl 38.61 & \chl 53.26 \\ 
  % & \chl \method (pretrained ProtMPNN-CMLM: \textit{fine-tune})           & \chl 6.9M/659M (\textbf{1.0\%})  & \chl 7.33 & \chl 6.83 & \chl 4.63 & \chl \textbf{36.47} & \chl \textbf{40.91} & \chl \textbf{54.62} \\
  % \cmidrule[0.2pt](lr){2-9}
  % & PiFold (reimpl.)                                                    & 6.6M/6.6M  &50.22  &- &-   \\ 

  &  \textsc{LM-Design} (\textit{w/} ProtMPNN encoder)              &  5.0M/659M   &  {54.41} &  0.88 &  77.07 \\
  & \chl \method (\textit{w/} ProtMPNN encoder)              & \chl 5.0M/659M   & \chl \textbf{54.54} & \chl 0.88 & \chl \textbf{77.12} \\
  \midrule
  % \cmidrule[0.5pt](lr){2-8}

  %%%%%%%%%%%%%%%%%%%%%%%%%%
  \multirowcell{4}{\rotatebox[origin=c]{90}{CATH 4.3}}
  & PiFold~\smallcitep{gao2022pifold} & 6.6M/6.6M    &  {51.66} & - & - \\ 
  & GVP-Transformer~\citep{hsu2022esmif}                                 & 142M/142M  &  51.60 &   - & -  \\
  &  \textsc{LM-Design} (\textit{w/} GVP-Trans encoder)          &  3.1M/302M  & 53.08 &  0.84 & 74.40  \\
  &  \textsc{LM-Design} (\textit{w/} GVP-Trans encoder)          &  6.3M/800M  & 56.49 &  0.85 & 74.89  \\
  %%%%%%%%%%%%%%% OURS %%%%%%%%%%%%%%%%%%%
  \cmidrule[0.5pt](lr){2-6}
  % & ProteinMPNN + CMLM                                                  & 1.9M/1.9M  & 6.31 &6.32  &4.85     & 40.30 & 39.02 &  48.25  \\ 
  % & \chl \method (ProtMPNN-CMLM)                                          & \chl 6.9M/659M (\textbf{1.0\%})  & \chl 5.96 &\chl 5.85 &\chl 4.48   & \chl 44.12  & \chl 44.05 &\chl 54.05 \\ 
  % & \chl \method (pretrained ProtMPNN-CMLM: \textit{fine-tune})           & \chl 6.9M/659M (\textbf{1.0\%})  & \chl 5.94 & \chl 5.80 & \chl 4.20 & \chl 45.65 & \chl \textbf{46.15} & \chl 55.92 \\
  % & \chl \method (pretrained ProtMPNN-CMLM: \textit{freeze})              & \chl 5.0M/659M (\textbf{0.7\%})  & \chl \textbf{5.88} & \chl \textbf{5.66} & \chl \textbf{4.19} & \chl \textbf{45.71} & \chl \textbf{46.15} & \chl \textbf{56.38} \\
  % \cmidrule[0.2pt](lr){2-9}
  % & PiFold (reimpl.)                                                    & 6.6M/6.6M  & 5.88& 5.55 & 4.47  & 42.86 &43.69& 50.68    \\ 
  & \chl \method (\textit{w/} GVP-Trans encoder)                                               & \chl 3.1M/302M   &\chl \textbf{53.27}   & \chl \textbf{0.85}  &\chl \textbf{75.31}  \\ 
  & \chl \method (\textit{w/} GVP-Trans encoder)                                               & \chl 6.3M/800M   &\chl \textbf{56.61}   & \chl \textbf{0.86}  &\chl \textbf{76.78}  \\ 
  
\bottomrule
\end{tabular}
}
   % \vspace{-8pt}
\end{table}
